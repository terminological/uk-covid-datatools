---
title: "Growth rate estimation"
author: "Rob Challen"
date: "25/06/2020"
output: 
  pdf_document :
    fig_caption: yes
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}    

knit: (function(inputFile, encoding,...) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "~/Dropbox/covid19/growth-rates/", output_file='growth-rates.pdf') })
fig_width: 7
fig_height: 5
out.width: "100%"
bibliography: current-rt.bib
csl: current-rt.csl
vignette: >
  %\VignetteIndexEntry{COVID-19 Growth rate estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

Robert Challen ^1,2^; Krasimira Tsaneva-Atanasova ^1,^3; Leon Danon ^3,4^;

1) EPSRC Centre for Predictive Modelling in Healthcare, University of Exeter, Exeter, Devon, UK.
2) Taunton and Somerset NHS Foundation Trust, Taunton, Somerset, UK.
3) The Alan Turing Institute, British Library, 96 Euston Rd, London NW1 2DB, UK.
4) Data Science Institute, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, UK. 

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
```

```{r setup}
library(tidyverse)

# devtools::load_all("~/Git/uk-covid-datatools/")
# devtools::install_github("terminological/uk-covid-datatools")
# library(ukcovidtools)
library(rgdal)
library(ggplot2)
library(ggspatial)
library(rgeos)
library(maptools)
library(lubridate)
library(patchwork)
library(sp)
devtools::load_all("~/Git/standard-print-output/")
ggplot2::theme_set(standardPrintOutput::defaultFigureLayout())
```

# Background

Our purpose in this paper is to demonstrate and test different methods of generating the growth rate of incidence of COVID-19. As we are producing phenomenological estimates for SPI-M we are limited somewhat in our approach, as we have no estimate of prevalence of COVID infection in the community. There are however a range of possible methods we have investigated for observing the growth rate from a purely observational standpoint. We have implemented these methods in our processing pipeline and summarise them here.

```{r}
devtools::load_all("~/Git/uk-covid-datatools/")
ukcovidtools::reload()
```

# Method

We calculate growth rates for two time series. Time series one is artificial, constructed from 100 time points starting form Jan 1st 2020 at 1, and growing exponentially with a day on day increase given by 1.5 for days 1-29 days, followed by 0.8 for 20-49, then 1.1 for 50-69 days, 0.7 for 70-89, then steady at 1 for the last 10 days. Timeseries 2 is based on case counts published on the Public Health England coronavirus website for England. It has a few missing values.

```{r}
testData = tibble(time = 1:200) %>% mutate(growth_rate = case_when(
  time < 60 ~ 0.05,
  time < 80 ~ -0.025,
  time < 140 ~ -0.025+0.045/40*(time-80),
  time < 180 ~ -0.05,
  TRUE ~ 0.03
), value = growth_rate ) %>% 
  mutate(type = "incidence", lambda_t = cumsum(growth_rate), date = as.Date("2020-01-01")+time) %>%
  group_by_all() %>%
  summarise(tibble(subgroup = 1:10,value=rpois(10,exp(lambda_t)))) %>%
  ungroup()

testTs = testData %>% mutate(statistic = "case",type = "incidence", code="XYZ", name="Test",codeType = "TEST",source="TEST",gender=NA_character_,ageCat=NA_character_)

ggplot(testTs,aes(x=date,y=value,group=subgroup))+geom_line(alpha=0.2)

testEst = testTs %>%
  tsp$logIncidenceStats(growthRateWindow = 28, nocache=TRUE) %>%
  tsp$estimateGrowthRate(growthRateWindow = 28, nocache=TRUE) %>%
  tsp$estimateGrowthRate2(growthRateWindow = 28, nocache=TRUE)
  
testEst3 = testTs %>%
  tsp$smoothAndSlopeTimeseries(smoothExpr = value,window = 28)
  

ggplot(testEst, aes(x=date, group=subgroup))+
  geom_line(aes(y=Growth.windowed.value,colour="dlog/dt (smoothed)"))+
  geom_ribbon(aes(ymin=Growth.windowed.value-1.96*Growth.windowed.SE.value,ymax=Growth.windowed.value+1.96*Growth.windowed.SE.value,fill="dlog/dt (smoothed)"),alpha=0.1)+
  geom_line(aes(y=growth_rate,colour="reference"))

ggplot(testEst, aes(x=date, group=subgroup))+
  geom_line(aes(y=Growth.value,colour="pois (loess)"))+
  geom_ribbon(aes(ymin=Growth.Quantile.0.025.value,ymax=Growth.Quantile.0.975.value,fill="pois (loess)"),alpha=0.1)+
  geom_line(aes(y=growth_rate,colour="reference"))


bayes.window=14
mu.prior = 0
var.prior = 1

testEst2 = testEst %>% 
  group_by(subgroup) %>% arrange(date) %>%
  mutate(
    subgroup.growth.mu = stats::filter(x = Growth.poisson/bayes.window, filter = rep(1,bayes.window)),
  ) %>%
  fill(subgroup.growth.mu, .direction = "down") %>%
  mutate(
    subgroup.growth.var = stats::filter(x = ((Growth.poisson-subgroup.growth.mu)^2)/(bayes.window-1), filter = rep(1,bayes.window))
  ) %>%
  #fill(subgroup.growth.var, .direction = "down") %>%
  mutate(  
    # https://towardsdatascience.com/a-bayesian-approach-to-estimating-revenue-growth-55d029efe2dd
    # bayes normal normal - prior mu = 0, prior variance = 1.
    subgroup.growth.mu.posterior =
      (mu.prior/var.prior + bayes.window*subgroup.growth.mu/subgroup.growth.var) /
      (1/var.prior+bayes.window/subgroup.growth.var),
    subgroup.growth.var.posterior = 1 /
      (1/var.prior + bayes.window/subgroup.growth.var)
  )

ggplot(testEst2 %>% filter(subgroup==2), aes(x=date, group=subgroup))+
  geom_line(aes(y=Growth.poisson,colour="pois (unsmoothed)"))+
  geom_ribbon(aes(ymin=Growth.poisson-1.96*Growth.SE.poisson,ymax=Growth.poisson+1.96*Growth.SE.poisson,fill="pois (smoothed)"),alpha=0.1)+
  geom_line(aes(y=growth_rate,colour="reference"))

ggplot(testEst2, aes(x=date, group=subgroup))+
  geom_line(aes(y=subgroup.growth.mu.posterior,colour="pois (bayes normal)"))+
  geom_ribbon(aes(ymin=subgroup.growth.mu.posterior-1.96*sqrt(subgroup.growth.var.posterior),ymax=subgroup.growth.mu.posterior+1.96*sqrt(subgroup.growth.var.posterior),fill="pois (bayes normal)"),alpha=0.1)+
  geom_line(aes(y=growth_rate,colour="reference"))+
  coord_cartesian(ylim=c(-0.1,0.1))

ggplot(testEst2, aes(x=date, group=subgroup))+
  geom_line(aes(y=Growth.windowed.poisson,colour="pois (smoothed)"))+
  geom_ribbon(aes(ymin=Growth.windowed.poisson-1.96*Growth.windowed.SE.poisson,ymax=Growth.windowed.poisson+1.96*Growth.windowed.SE.poisson,fill="pois (smoothed)"),alpha=0.1)+
  geom_line(aes(y=growth_rate,colour="reference"))+
  coord_cartesian(ylim=c(-0.1,0.1))

ggplot(testEst3, aes(x=date, group=subgroup))+
  geom_line(aes(y=Ratio.value,colour="dI/dt / I"))+
  geom_ribbon(aes(ymin=Ratio.value-1.96*Ratio.SE.value,ymax=Ratio.value+1.96*Ratio.SE.value,fill="dI/dt / I"),alpha=0.1)+
  geom_line(aes(y=growth_rate,colour="reference"))+
  coord_cartesian(ylim=c(-0.1,0.1))


ukts = dpc$datasets$getPHEDashboard() %>% 
  filter(name=="England" & type=="incidence") %>%
  tsp$logIncidenceStats(window=14) %>%
  tsp$estimateLittleR(window = 14, nocache=TRUE)
```

```{r fig1, fig.cap="Epidemic curve based on (A) an artifical and (B) a real timeseries of lab positive cases in England as published on the PHE dashboard, on a log1p scale."}
p1=ggplot(testTs,aes(x=date,y=value))+geom_bar(stat="identity", width=0.6)+scale_y_continuous(trans="log1p")+labs(subtitle="Artificial")
p2= ggplot(ukts,aes(x=date,y=value))+geom_bar(stat="identity", width=0.6)+scale_y_continuous(trans="log1p")+labs(subtitle="PHE England cases")
(p1+p2+patchwork::plot_annotation(tag_levels = "A") +patchwork::plot_layout(nrow=1)) %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/growth-rates/Fig1_EpidemicCurves")
```

The rate of growth of these distributions is calculated using 3 methods all of which are applied over a rolling window. In all 3 methods the same 14 day window has been chosen, as the real data has a strong weekly cycle and windows shorter than 14 days become unstable.

The 3 methods explored are Poisson distribution fitting, direct estimation of the rate of change incidence as a fraction of incidence, and estimation of the rate of change of the logarithm of incidence.

Poisson fitting is conducted using the R glm library on a 14 day subsection of the data. The rate of change of the log of the rate parameter wrt to time of the Poisson distribution is used as an estimate of the growth rate.

The second method, direct estimation of the rate of change assumes that the incidence is a noisy sample of a smooth function. Estimation of the value and first derivative of that smooth function is done by applying a local regression direct to incidence figures. The regression is linear over the 14 day window, and performed with the locfit R package. The estimate of the growth rate is the estimated gradient at any given timepoint divided by estimated value. The fitting is conducted with a degree 1 polynomial, the locfit alpha parameter is set to ensure that fitting algorithm is using the correct numbers of points of data (alpha = window/N in a timeseries of N points).

$$
r = \frac{\delta I_t}{\delta t}I_t^{-1}
$$

The third method assumes that the incidence curve is a smooth function obeying exponential growth dynamics. To estimate this the incidence curve is shifted by 1 and log transformed, and the slope of this curve is estimated, again using the locfit library, with degree 1 and alpha determined as above. The slope of this curve is a direct estimate of the growth rate.

$$
I_{t+\delta t} \sim I_t(1+e^{r\delta t})
$$
$$
r = \frac{\delta}{\delta t} log(I_t+1)
$$
Both approaches 2 & 3 rely on smoothing of the discrete incidence values to a continuous function. These both can be affected by outlying data points, so prior to calculation of the local gradient outlier detection must be applied. This is currently being done using the forecast R package (tsclean) which detects and imputes local values for outlying and missing data in a time series.

# Results

TODO: update this with artificial time-series from projections package

```{r}

#install.packages("projections")

seed = incidence::incidence(dates = as.Date(unlist(lapply(as.Date("2020-01-01"):as.Date("2020-01-10"),function(x) rep(x,runif(1,90,110)))),"1970-01-01"))
proj_3 <- projections::project(x = seed, R = c(1,1.2,0.7), si = Flu2009$si_distr, n_days = 60, time_change = c(20,40), R_fix_within = TRUE)

plot(proj_3)

tmp = as.data.frame(proj_3,long=TRUE) %>% group_by(sim) %>% mutate(incidence = as.numeric(incidence))
```

Figure 2 shows the 3 methods applied to an artificial time series with defined growth rate. In the panel A we see the Poisson method closely estimates the artifical series with a predictable delay introduced by the windowing. The implementation estimates growth rates using values prior to the point of estimation. 

In panel B we see estimates fo the growth rate using a direct method. As mentioned above this method is dependent on the determination of an underlying function, which is assumed to be smooth. The outlier detection algorithm has kicked in (turquoise points in the rug plot) due to the excessive spikiness of the exponential growth phase the early part of the artificial time series.

In panel C we have the estimates based on local exponential fitting. This performs in a similar fashion to A but the timing of the delay is variable as the locally fitted exponential will use points ahead as well as points behind when these are available. This has the somewhat undesirable effect to compressing the resulting time series towards the end. This is less affected by the imputation algorithm as the log incidence curve is less spiky but around the transitions we can again see that the outlier detection function has made changes to the raw data. We need to continue investigation on this.

```{r fig2, fig.cap="Comparison of 3 methods for growth rate estimation in an artificial time series."}


p1 = ggplot(testTs, aes(x=date)) +geom_line(aes(y=log(growth_rate)), colour = "black")+ plotRibbons(`Mean(Growth rate)`,`Sd(Growth rate)`, "red")+labs(y="r",subtitle="Poisson \U03BB")+coord_cartesian(ylim=c(-1,1))
p2 = ggplot(testTs, aes(x=date)) +geom_line(aes(y=log(growth_rate)), colour = "black")+ plotRibbons(`Ratio.value`,`Ratio.SE.value`, "blue")+geom_rug(aes(colour=Imputed.value),show.legend = FALSE)+labs(y="r",subtitle=expression(frac(delta*I,delta*t) * I^-1))+coord_cartesian(ylim=c(-1,1))
p3 = ggplot(testTs, aes(x=date)) +geom_line(aes(y=log(growth_rate)), colour = "black")+ plotRibbons(`Growth`,`Growth.SE`,"magenta")+geom_rug(aes(colour=`Imputed.log(value + 1)`),show.legend = FALSE)+labs(y="r",subtitle=expression(frac(delta*log(I+1),delta*t)))+coord_cartesian(ylim=c(-1,1))

(p1+p2+p3+patchwork::plot_annotation(tag_levels = "A") + patchwork::plot_layout(nrow=1)) %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/growth-rates/Fig2_ArtificalTest")

```

Figure 3 shows the same 3 algorithms applied to the real incidence curves in figure 1B. 

In panel A the Poisson fitting algorithm demonstrates narrow confidence intervals compared to stability of the underlying value. This is reflecting the reality of reporting delay, but this is despite us employing a 14 day window. The method could be seen as over sensitive, and we also seen this in time series generated with EpiEstim unless they are heavily smoothed.

In panel B the direct estimation method again demonstrates some value but is influenced by our outlier detection algorithm which has been triggered by the noise at the height of the peak, and in the early fall, this has produced an inappropriately smooth and high confidence area. 

In panel C the local fitting of exponential produces a smooth estimate with appropriate confidence intervals. It is difficult to say whether it is over smoothed. We can empirically adjust windowing functions to see what effect that will have in the future.

```{r fig3, fig.cap="Comparison of 3 smoothing methods in a real time series."}
p1 = ggplot(ukts, aes(x=date)) + plotRibbons(`Mean(Growth rate)`,`Sd(Growth rate)`, "red")+labs(y="r",subtitle="Poisson \U03BB")+coord_cartesian(ylim=c(-0.25,0.5))
p2 = ggplot(ukts, aes(x=date)) + plotRibbons(`Ratio.value`,`Ratio.SE.value`, "blue")+geom_rug(aes(colour=Imputed.value),show.legend = FALSE)+labs(y="r",subtitle=expression(frac(delta*I,delta*t) * I^-1))+coord_cartesian(ylim=c(-0.25,0.5))
p3 = ggplot(ukts, aes(x=date)) + plotRibbons(`Growth`,`Growth.SE`,"magenta")+geom_rug(aes(colour=`Imputed.log(value + 1)`),show.legend = FALSE)+labs(y="r",subtitle=expression(frac(delta*log(I+1),delta*t)))+coord_cartesian(ylim=c(-0.25,0.5))

(p1+p2+p3+patchwork::plot_annotation(tag_levels = "A") + patchwork::plot_layout(nrow=1)) %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/growth-rates/Fig3_RealTest")

```

# Bayesian method

```{r}
# https://people.stat.sc.edu/Hitchcock/slides535day5spr2014.pdf
# https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions
# https://people.stat.sc.edu/Hitchcock/stat535slidesday18.pdf

# assume lambda is fixed over a short window
# and lambda is a gamma distributed quantity with ?prior mean and ?SD
# posterior lambda is gamma(Sum(xi) + α, n + β)

# gr is log of ratio of lambda at time t and lambda at t+1
# https://stats.stackexchange.com/questions/207595/distribution-of-the-ratio-of-two-gamma-random-variables
# https://rdrr.io/cran/extraDistr/src/R/beta-prime-distribution.R


```


# Conclusion

We've presented 3 methods for estimating the growth rates, which produce comparable estimates to each other which are in line with those expected in an artificial time series. As we have been asked to produce estimates of growth rate, in the short term we will the method based on local fitting of exponential growth as these seem to have a good trade off between signal and noise. It is clear that we need to revisit our outlier detection algorithm as this is having excessive influence particularly in smoothing the peak of the incidence curve. This is the only place where it is used in this way - and in our estimates of R(t) the outlier detection is used on the log1p data, where it is not triggered. It's worth also clarifying that a less invasive smoothing algorithm is employed during Rt estimation, and this is under continuous review.

# Limitations

None of these methods account for estimates of the prevalence, and hence a "true" growth rate from an epidemic sense. The estimates of growth rate will tend to be biased on the low side during the growth phase of the epidemic. For the methods based on local fitting algorithm it is possible that weighting the local fitting using a convolution of the serial interval distribution could address this.

