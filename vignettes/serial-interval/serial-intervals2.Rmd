---
title: "Meta-analysis of the SARS-CoV-2 serial interval and the impact of parameter uncertainty on the COVID-19 reproduction number"
output:
  pdf_document :
    fig_caption: yes
knit: (function(inputFile, encoding,...) {
  options("spo.hidefigures"=FALSE);
  rmarkdown::render(
    inputFile,
    encoding = encoding,
    output_dir = "~/Dropbox/covid19/serial-interval/", output_file=paste0('serial-intervals-',Sys.Date(),'.pdf'))
  })
# output:
#   word_document :
#     fig_caption: yes
#     fig_width: 7
# knit: (function(inputFile, encoding,...) {
#   options("spo.hidefigures"=TRUE);
#   rmarkdown::render(
#     inputFile,
#     encoding = encoding,
#     output_dir = "~/Dropbox/covid19/serial-interval/", output_file=paste0('serial-intervals-',Sys.Date(),'.docx'))
#   })
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
fig_width: 7
fig_height: 5
out.width: "100%"
bibliography: serial-interval.bib
csl: sage-vancouver.csl
vignette: >
  %\VignetteIndexEntry{COVID-19 Serial Intervals}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align="center"
)
```

```{r setup}
library(tidyverse)

# devtools::load_all("~/Git/uk-covid-datatools/")
# devtools::install_github("terminological/uk-covid-datatools")
# library(ukcovidtools)
# library(rgdal)
library(ggplot2)
library(patchwork)
devtools::load_all("~/Git/standard-print-output/")
ggplot2::theme_set(standardPrintOutput::defaultFigureLayout())

cap = list(
  fig = captioner::captioner(prefix="Figure"),
  tab = captioner::captioner(prefix="Table"),
  sfig = captioner::captioner(prefix="Supplemental figure"),
  stab = captioner::captioner(prefix="Supplemental table")
)

ref = list(
  fig = function(name) cap$fig(name,display="cite"),
  tab = function(name) cap$tab(name,display="cite"),
  sfig = function(name) cap$sfig(name,display="cite"),
  stab = function(name) cap$stab(name,display="cite")
)

ukcovidtools::setup()

distLabel = function(fit) {
  fit$printDistributionSummary() %>% select(dist,param,mean) %>% pivot_wider(names_from = param, values_from = mean) %>% ungroup() %>% summarise(label = paste0(sprintf("%s: %1.1f (\u00B1%1.1f) days",dist,mean,sd),collapse = "\n"))
}

# This is needed as the code saves some rData files and otherwise this gets into the final output document.
options("usethis.quiet"=TRUE)

```

```{r}

serialIntervals = readr::read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vRdVV2wm6CcqqLAGymOLGrb8JXSe5muEOotE7Emq9GHUXJ1Fu2Euku9d2LhIIK5ZvrnGsinH11ejnUt/pub?gid=0&single=true&output=csv")

## Meta-analysis: ----

metaParams = dpc$getSaved("SERIAL-INTERVAL-META", orElse = function() {
  tmp = serialIntervals %>% mutate(yi = mean_si_estimate, sei = (mean_si_estimate_high_ci-mean_si_estimate_low_ci)/3.92) %>% filter(!is.na(sei)) %>% filter(assumed_distribution == "gamma" & estimate_type %>% stringr::str_starts("serial"))
  meanfit = suppressWarnings(metaplus::metaplus(tmp$yi, tmp$sei, slab=tmp$label, random="mixture"))
      
  tmp2 = serialIntervals %>% mutate(yi = std_si_estimate, sei = (std_si_estimate_high_ci-std_si_estimate_low_ci)/3.92) %>% filter(!is.na(sei)) %>% filter(assumed_distribution == "gamma" & estimate_type %>% stringr::str_starts("serial"))
  sdfit = suppressWarnings(metaplus::metaplus(tmp2$yi, tmp2$sei, slab=tmp2$label, random="mixture"))
  
  serialIntervalMetaAnalysis = tribble(
    ~param, ~mean, ~sd, ~lower, ~upper,
    "mean", meanfit$results["muhat",1], NA, meanfit$results["muhat",2], meanfit$results["muhat",3],
    "sd", sdfit$results["muhat",1], NA, sdfit$results["muhat",2], sdfit$results["muhat",3]
  ) %>% mutate(dist="gamma")
  
  # save the data:
  usethis::use_data(serialIntervalMetaAnalysis, overwrite = TRUE)
  return(
    serialIntervalMetaAnalysis
  )
})

si2 = SerialIntervalProvider$metaAnalysis(dpc, metaDf = metaParams)
metaSIresult = si2$getSummary() %>% summarise(label = sprintf("mean %1.1f (95%% CrI %1.1f\U2013%1.1f) and SD %1.1f (95%% CrI %1.1f\U2013%1.1f) days",meanOfMean,minOfMean,maxOfMean,meanOfSd,minOfSd,maxOfSd))

```

```{r}
## FF100: ----  

si3 = SerialIntervalProvider$fromFF100(dpc)


```



```{r}
## Resampling: ----
#rm(tmp,tmp2,tmp3)

resamples = dpc$getSaved("SERIAL-INTERVAL-RESAMPLE", orElse = function() {

  shortestCredibleSI = -7
  longestCredibleSI = 28
  samples=100
  boot.samples = NULL
  set.seed(101)
      # bootIterations = 250
      
  dfit = DistributionFit$new()
      
  parameterisedSIs = serialIntervals %>% 
    filter(
      estimate_type %>% stringr::str_starts("serial") &
      !assumed_distribution %in% c("empirical","unknown"),
    ) %>% 
    group_by(i = row_number()) %>% 
    group_modify(function(d,g,...) {
      paramDf = tribble(
        ~param, ~mean, ~sd, ~lower, ~upper,
        "mean", d$mean_si_estimate, (d$mean_si_estimate_high_ci - d$mean_si_estimate_low_ci)/3.96, d$mean_si_estimate_low_ci, d$mean_si_estimate_high_ci,
        "sd", d$std_si_estimate, NA, d$std_si_estimate_low_ci, d$std_si_estimate_high_ci
      )
      dfit$withSingleDistribution(dist = d$assumed_distribution, paramDf = paramDf, bootstraps = samples, N=d$sample_size, source = g$i)
      tibble()
      # side effect in group modify. This deserves 100 Hail Mary's
  })
      
  dfit$generateSamples(sampleExpr = N, seed=101)
  sampleDf = dfit$samples %>% select(bootstrapNumber, value, N, source)
      
  # include raw data for Xu et al, who did not produce parameterised estimates - only empirical distribution.
  # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7276042/bin/73153-2020.03.02.20029868-1.xlsx
  xuXls = dpc$datasets$download(id = "XU_SERIAL_INTERVAL",url = "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7276042/bin/73153-2020.03.02.20029868-1.xlsx", type = "xlsx")
  xuData = readxl::read_excel(xuXls)
  xuSamples = suppressWarnings(xuData %>% mutate(dt = as.integer(`p_Date of onset`)-as.integer(`s_Date of onset`)) %>% filter(!is.na(dt)) %>% pull(dt))
  xuSrc = max(sampleDf$source)+1
  
  # bootstrapping 90% of total into 100 samples
  N = length(xuSamples)*0.9
  set.seed(101)
  for(i in 1:samples) {
    # raw data also available from Du et al
    # https://github.com/MeyersLabUTexas/COVID-19/blob/master/Table%20S5%20medrxiv.xlsx?raw=true
    # their analysis is included though as resulted in normal distribution
    # N.B. it is probably the same data as Xu
    sampleDf = sampleDf %>% bind_rows(tibble(
      bootstrapNumber = i, 
      value = sample(xuSamples, size=N),
      N = N,
      source = xuSrc
    ))
  }
      
  # We can now: 
  # use this to directly estimate a parameterised distribution for the serial interval with or without shift
  # Or create a set of discretised empirical distributions for epiestim to explore.
  out = sampleDf %>% ungroup() #group_by(N,source)
  serialIntervalResampling = out %>% filter(value > shortestCredibleSI & value <= longestCredibleSI)
  usethis::use_data(serialIntervalResampling, overwrite = TRUE)
  return(out)
})

si1 = SerialIntervalProvider$resampledSerialInterval(dpc, resamples=resamples)
resampleSIresult = si1$getSummary() %>% summarise(label = sprintf("mean %1.1f (95%% CrI %1.1f\U2013%1.1f) and SD %1.1f (95%% CrI %1.1f\U2013%1.1f) days",meanOfMean,minOfMean,maxOfMean,meanOfSd,minOfSd,maxOfSd))

```

Robert Challen^1,2^; Ellen Brooks-Pollock^3^; Krasimira Tsaneva-Atanasova^1,4,5^; Leon Danon^3,4,5^

1) EPSRC Centre for Predictive Modelling in Healthcare, University of Exeter, Exeter, Devon, UK.
2) Somerset NHS Foundation Trust, Taunton, Somerset, UK.
3) Bristol Medical School, Population Health Sciences, University of Bristol, Bristol, UK.
4) The Alan Turing Institute, British Library, 96 Euston Rd, London NW1 2DB, UK.
5) Data Science Institute, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, UK. 


# Abstract

***The serial interval of an infectious disease, commonly interpreted as the time between onset of symptoms in sequentially infected individuals within a chain of transmission, is a key epidemiological quantity involved in estimating the reproduction number. The serial interval is closely related to other key quantities, including the incubation period, the generation interval (the time between sequential infections) and time delays between infection and the observations associated with monitoring an outbreak such as confirmed cases, hospital admissions and deaths. Estimates of these quantities are often based on small data sets from early contact tracing and are subject to considerable uncertainty, which is especially true for early COVID-19 data. In this paper we estimate these key quantities in the context of COVID-19 for the UK, including a meta-analysis of early estimates of the serial interval. We estimate distributions for the serial interval with a `r resampleSIresult` (empirical distribution), the generation interval  with a mean 4.8 (95% CrI 4.3--5.41) and SD 1.7 (95% CrI 1.0--2.6) days (fitted gamma distribution), and the incubation period with a mean 5.5 (95% CrI 5.1--5.8) and SD 4.9 (95% CrI 4.5--5.3) days (fitted log normal distribution). We quantify the impact of the uncertainty surrounding the serial interval, generation interval, incubation period and time delays, on the subsequent estimation of the reproduction number, when pragmatic and more formal approaches are taken. These estimates place empirical bounds on the estimates of most relevant model parameters and are expected to contribute to modelling COVID-19 transmission.***

Competing interests: Support for RC and KTAâ€™s research is provided by the EPSRC via grant EP/N014391/1, RC is also funded by TSFT as part of the NHS Global Digital Exemplar programme (GDE); no financial relationships with any organisations that might have an interest in the submitted work in the previous three years, no other relationships or activities that could appear to have influenced the submitted work.

Funding: RC and KTA gratefully acknowledges the financial support of the EPSRC via grant EP/N014391/1 and NHS England, Global Digital Exemplar programme. LD and KTA gratefully acknowledges the financial support of The Alan Turing Institute under the EPSRC grant EP/N510129/1. LD and EBP are supported by Medical Research Council (MRC) (MC/PC/19067). EBP was partly supported by the NIHR Health Protection Research Unit in Behavioural Science and Evaluation at University of Bristol, in partnership with Public Health England (PHE).

Authors contributions: All authors discussed the concept of the article and RC wrote the initial draft. KTA, EB-P and LD commented and made revisions. All authors read and approved the final manuscript. RC is the guarantor. The views presented here are those of the authors and should not be attributed to TSFT or the GDE.

# Introduction

Since the end of 2019, the novel strain of coronavirus, SARS-Cov-2 has caused a global pandemic of disease. The speed with which the virus spreads is dependent on biological determinants that enable viral replication within individuals and onward transmission to others. The minimal set of parameters required for understanding the dynamics of any novel infectious disease pathogen include the potential for transmission, the duration of infectiousness (often captured in models as a recovery rate) and the generational interval: the time between two subsequent cases in a chain of infection. 

Although much more is understood about these basic parameters than at the beginning of the pandemic, substantial uncertainty remains over the future trajectory of the disease which depends partly on the interventions that are implemented. 

The best possible evidence on each parameter determining the dynamics of the epidemic is crucial for choosing the most appropriate interventions. Estimating the generation interval is particularly challenging due to the fundamentally hidden nature of transmission events. In `r ref$fig("infection-timeline")` we summarise the timeline of key events for two adjacent infected individuals (infector and infectee) in a chain of transmission. 

As described by Svensson et al.[@svenssonNoteGenerationTimes2007] the generation interval is defined as the time between the infection of an infector and infectee, and in practice is not easy to observe, as an infection goes through some latent period, during which it is undetectable[@haoReconstructionFullTransmission2020], and a pre-symptomatic phase during which an individual may be infectious, and possibly detectable through screening, before the disease manifests with clinical symptoms. The latent period and pre-symptomatic phase are together usually referred to as the incubation period (*T~incubation~*). From the onset of symptoms, the diagnosis will be confirmed by some canonical test after some time delay (*T~onsetâ†’test~*), later the patient may require admission to hospital (*T~onsetâ†’admission~*), or may die (*T~onsetâ†’death~*). These subsequent events clearly may not happen in that order, and even diagnosis may occur after death. The time between these key events (onset of symptoms, test confirmation of case, hospital admission, and death) for two sequentially infected people in a chain of transmission are described as serial intervals[@svenssonNoteGenerationTimes2007], although in general usage, and in the rest of this paper, the term "serial interval" is taken to mean the interval between onset of symptoms (*SI~onset~*). In `r ref$fig("infection-timeline")` and the rest of this paper we use the terms "case interval" (*SI~case~*), "admission interval" (*SI~admission~*), and "death interval" (*SI~death~*) to describe the other intervals. The generation interval is by definition a non-negative quantity, but all the other measures may be negative if the variation of the delay from the event of infection from person to person exceeds the period between infections. This is more likely for death interval than for serial interval, and for diseases with long pre-symptomatic periods, for example HIV[@beckerMethodNonparametricBackprojection1991].

![](Fig1_InfectionTimeline.png) 

`r cap$fig("infection-timeline","A timeline of events associated with a single infector-infectee pair in a transmission chain")`

The reproduction number (*R~t~*) is a key measure of the state of the epidemic, and estimates of *R~t~* can become a determinant of public health policy. The "renewal equation" method for estimating *R~t~*, depends on a time series of infections, and on the infectivity profile - a measure of the probability that a secondary infection occurred on a specific day after the primary case, given a secondary infection occurred[@coriEstimateTimeVarying; @thompsonImprovedInferenceTimevarying2019]. A Bayesian framework is then used to update a prior probabilistic estimate of *R~t~* on any given day with both information gained from the time series of infections in the epidemic to date and the infectivity profile to produce a posterior estimate of *R~t~*. In both the original[@coriNewFrameworkSoftware2013a] and revised[@thompsonImprovedInferenceTimevarying2019] implementations of this method, the authors acknowledge the pragmatic use of the serial interval distribution, as a proxy measure for the infectivity profile, and the incidence of symptom onset or case identification as a proxy for the incidence of infection, with the caveat that these introduce a time lag into the estimates of *R~t~*. It has been noted by various authors that use of the serial interval as a proxy for infectivity profiles is a pragmatic choice[@coriNewFrameworkSoftware2013a; @thompsonImprovedInferenceTimevarying2019] but can introduce a bias into estimates of *R~t~*[@brittonEstimationEmergingEpidemics2019; @gosticPracticalConsiderationsMeasuring2020a]. 

In the COVID-19 outbreak a limited number estimates of the serial interval are available from studies of travelers from infected areas, and early contact tracing studies (detailed in `r ref$tab("lit-review")`). The infectivity profile is comprised of non-negative values by definition. The serial interval, on the other hand, can be measured as negative for several reasons. For example,  if the incubation period of the infector is at the short end of the distribution and that of the infectee is at the tail, a negative serial interval would be observed. Negative values have been noted as a feature in at least one estimate of the serial interval of SARS-CoV-2 to date[@xuHouseholdTransmissionsSARSCoV22020], but cannot be sensibly used in estimates of *R~t~*.

Direct measurement of the serial interval distribution is further complicated by the fact that symptom onset is often not observed due to the scale of the outbreak and that most infection is asymptomatic[@daviesAgedependentEffectsTransmission2020; @mizumotoEstimatingAsymptomaticProportion2020]. The data available during an epidemic are a time series of counts of observations, such as confirmed cases (test results confirming diagnosis), hospital admissions, and deaths. As depicted in `r ref$fig("infection-timeline")` these events occur after infection, following a period of time. Therefore the observed time series of observed cases is a result of the unobserved time series of infections governed by the serial interval, convolved by the distribution of the time delay from infection to case identification.

Since transmission model-based inference is predicated on infections best practice would be to use the generation interval and infer the unobserved incidence of infection from the observations we have using back propagation or de-convolution[@gosticPracticalConsiderationsMeasuring2020a; @beckerMethodNonparametricBackprojection1991]. A pragmatic alternative[@gosticPracticalConsiderationsMeasuring2020a] is to simply calculate *R~t~* using a serial interval and un-adjusted case numbers with a simple correction for the time delay between infection and cases by shifting our observed cases in time. To do either of these things we need estimates of the time delays between infection and symptom onset (incubation period), infection and case identification (test), infection and admission, and between infection and death, and their distributions. 

The purpose of this paper is to determine the best estimates for the parameters we need to calculate *R~t~* for the UK using either formal or pragmatic approaches. We also wish to understand the degree and nature of bias introduced by using the pragmatic approach using truncated serial interval distributions as approximations for infectivity profile[@brittonEstimationEmergingEpidemics2019], and case numbers as proxy for infection incidence, compared to the more formal methods of using generation intervals and inferred infection numbers[@gosticPracticalConsiderationsMeasuring2020a]. To do this we investigate estimates for the serial interval, the incubation period, and then time between symptom onset and case identification, admission or death, in the UK. From these we infer the generation interval, and time between infection and case identification, admission and death. With all these parameters available we then compare the impact of using pragmatic and more formal approaches on estimation of *R~t~*.

# Methods

## Serial interval estimation

Firstly, we conducted a literature review for studies that describe serial interval estimates using PubMed and the search terms "(SARS-CoV-2 or COVID-19) and 'Serial interval'" and reviewed the abstracts of relevant original research papers. These were compared to papers reported on the MIDAS Online Portal for COVID-19 Modeling Research[@MIDASOnlinePortal], and with existing meta-analyses[@zhangMetaanalysisSeveralEpidemic2020]. 
From these papers, serial interval mean and standard deviation estimates were extracted, along with information about assumed statistical distributions, and the sample size of the study. A random effects meta-analysis was conducted[@beathMetaplusPackageAnalysis2016a] on the subset of papers that reported confidence intervals, and which assumed gamma distributions for their estimates of serial interval. This excluded a number of larger studies, and we had concerns over potential violations of the assumptions underpinning the meta-analysis, most notable the assumption of normal distribution of effect size[@jacksonWhenShouldMetaanalysis2018; @tierneyPracticalMethodsIncorporating2007; @veronikiMethodsEstimateBetweenstudy2016], so we also undertook a re-sampling exercise, as described below.

For studies that reported a probability distribution for the serial interval, we randomly selected one hundred probability distributions consistent with those reported in each paper, assuming a normally distributed mean (central limit), and a chi-squared distributed standard deviation with degrees of freedom determined by the sample size of the study. From this family of probability distributions we created random samples based on the original sample size[@adamsResamplingTestsMetaAnalysis1997; @chuangHybridResamplingMethods2000]. For empirical data reported in the studies we obtained the data where available and used 100 random bootstrap sub-samples with replacement, to a relative size determined by the original sample size of the study. The empirical and distribution based samples were combined into 100 groups of samples, with each group containing representation of each source article with numbers proportional to the size of the original study. A normal, negative binomial and gamma probability distributions were then fitted to these 100 groups using maximum likelihood estimation, implemented in R[@rcoreteamLanguageEnvironmentStatistical2017; @delignette-mullerFitdistrplusPackageFitting2015a], giving us both parametric probability distribution estimates and 100 empirical probability distributions estimates of the combination of all the source studies (from here on referred to as the "re-sampled serial interval estimate").

Second, we use data collected under the "First Few Hundred" (FF100) case protocol by Public Health England[@publichealthenglandFirstFewHundred; @boddingtonCOVID19GreatBritain2020] which provides a limited number of linked cases of proven transmission, mostly within households, and interval censored symptom onset dates. To this data we fitted normal, gamma and negative binomial distributions using the same methodology as above. When fitting gamma and negative binomial distributions we truncated the data at zero, to prevent negative values, and for the gamma distributions we required that the shape parameter had a lower bound of 1, which enforces that the distribution density is zero at time zero. 

## Incubation period estimation

The incubation period has been previously estimated by Lauer et al. and Sanche et al.[@lauerIncubationPeriodCoronavirus2020; @sancheHighContagiousnessRapid] for China in the early phase of the epidemic. The FF100 data contains interval censored exposure data coupled to symptom onset; using this we derived a UK specific estimate to assess if it was consistent. Furthermore, the Open COVID-19 Data Working Group[@kraemer2020epidemiological;@xuOpenAccessEpidemiological2020] provides a large international data set which includes some travel history and symptom onset data. As the incubation period is a key quantity in the analysis, we reassessed earlier estimates[@lauerIncubationPeriodCoronavirus2020; @sancheHighContagiousnessRapid] of the incubation period with this larger data set. For both datasets we fitted gamma, weibull, log normal, and negative binomial probability distributions to the intervals between putative exposure and symptom onset, accounting for censoring where present, to estimate the incubation period distribution. 

## Generation interval estimation

The generation interval is the fundamental variable for modelling transmission. Under the assumption that it follows a gamma distribution we can infer its parameters using the serial interval (*SI~onset~*) distribution, the incubation period distribution, and the constraint that the generation interval is a non-negative quantity. Using the best estimate of the incubation period distribution, we combined random samples from a parameterised probability distribution for the generation interval with two samples from the incubation period to satisfy the following relationship, thus simulating the serial interval:

$$
\begin{aligned}
SI_{onset,A\rightarrow B} &= SI_{infection,A\rightarrow B} + T_{incubation,B} - T_{incubation,A}\\
E[SI_{onset}] &= E[SI_{infection} + T_{incubation} - T_{incubation}]
\end{aligned}
$$

The mean and standard deviation of the simulated serial interval distribution were then compared to the empirical re-sampled serial interval distributions we estimated in an earlier stage. The parameters for the generation interval distribution were then optimized by a recursive linear search on the standard deviation, with the constraints that the mean of the simulated and empirical distributions must be the same[@brittonEstimationEmergingEpidemics2019], and the standard deviation must be smaller than the mean (ensuring the gamma function scale parameter is larger than one and hence the density is zero at time zero). The minimization function we employed was the absolute difference in inter-quartile ranges of simulated and observed distributions. This process was repeated for 100 different simulated samples that were compared to the 100 different empirical re-sampled serial interval estimates from the previous stage of our analysis to get confidence intervals on our estimates of the generation interval distribution.

## Impact of using the serial interval on estimation of *R~t~*

With various estimates of serial interval and generation interval we wished to understand the qualitative impact this variation might have on our estimates of *R~t~*. To investigate this we used the forward equation approach implemented in the R library EpiEstim[@thompsonImprovedInferenceTimevarying2019a; @coriEstimateTimeVarying; @coriNewFrameworkSoftware2013a]. We estimate values of *R~t~* for 4 time points in the first wave of the COVID-19 pandemic in England representative of the ascending phase, the peak, the early descending phase and the late descending phase. We used data retrieved from the Public Health England API[@CoronavirusCOVID19UK; @CoronavirusCOVID19UKa]. For this analysis we assume the infectivity profile can be represented using a parameterised gamma distribution, and estimate *R~t~* for a wide range of combinations of mean and standard deviation, using a fixed calculation window of 7 days, at each of our four time points. The resulting relationship between *R~t~*, mean infectivity profile, and standard deviation of infectivity profile were compared visually.

## Time delays from infection to case identification, admission, and death

Estimation of the time interval between the onset of symptoms and the observations of positive test result, hospital admission, and death was performed (*T~onsetâ†’test~*, *T~onsetâ†’admission~*, and *T~onsetâ†’death~*) using the CHESS data set[@CoronavirusCOVID19Using]. The CHESS data set is hospital based, and was initially limited to intensive care admissions, but a subset of hospitals have reported all admissions, and this is what we focused on. Within the CHESS data set there are a set of patients who have symptom onset dates recorded, dates that a specimen was taken that subsequently was tested positive, hospital admission date, and date of death, if the patient died. The time delays from symptom onset to the different observations were calculated and fitted to probability distributions using the R library fitdistrplus[@delignette-mullerFitdistrplusPackageFitting2015a] as described above.

The time delay from infection to observation was obtained by combining our estimate of the incubation period distribution from the Open COVID-19 Data Working Group data set[@kraemer2020epidemiological;@xuOpenAccessEpidemiological2020] with onset to observation delays from the CHESS data set[@CoronavirusCOVID19Using] using the following relationship.

$$
\begin{aligned}
T_{infection \rightarrow observation} &= T_{infection \to onset} + T_{onset \rightarrow observation} \\
&= T_{incubation} + T_{onset \rightarrow observation}
\end{aligned}
$$

We combined the incubation period and onset to observation distributions using a random sampling approach, assuming independence of the two variables. These random samples were then estimated as parameterised statistical distributions in the same manner as above, with the constraint that all the time delays from infection to observation are non-negative quantities, and their probability is zero at time zero.

## Impact of deconvolution

In the final piece of our analysis we sought to qualitatively compare estimates of *R~t~* based on data for England from the Public Health England API[@CoronavirusCOVID19UK; @CoronavirusCOVID19UKa], in each of the following two scenarios.

Firstly, we based *R~t~* estimates on observational data (cases, admissions, deaths) as a proxy for infection events, and used truncated serial interval distribution as a proxy for infectivity profile. A simple-but-incorrect adjustment to the dates of these *R~t~* estimates was made to align the estimate to date of infection rather than date of observation.

Secondly, using the time delay distributions from the previous stage, we used de-convolution to infer a set of time series of infections from the same observational data and used our estimate of the generation interval as a proxy for the infectivity profile. This second approach has been recommended by Gostic et al[@gosticPracticalConsiderationsMeasuring2020a]. To do this we applied a non parametric back projection algorithm from the surveillance R package[@meyerSpatioTemporalAnalysisEpidemic2017], based on work by Becker et al.[@beckerMethodNonparametricBackprojection1991] and Yip et al.[@yipReconstructionInfectionCurve2008], to infer three putative infection time series from observed cases, admissions or deaths. The inferred time series were then used to estimate *R~t~* through EpiEstim using the parametric gamma distributed estimate of the generation interval from above. In applying the de-convolution we discovered it requires a full time series beginning with zero cases for sensible results and this required we impute the early part of the hospital admission time series, which we did by assuming an early constant exponential growth phase.

The resulting *R~t~* time series were compared qualitatively.

# Results

## Serial interval estimation

Our PubMed search retrieved 62 search hits of which 12 were original research articles containing estimates of serial intervals[@biEpidemiologyTransmissionCOVID192020a; @ceredaEarlyPhaseCOVID192020a; @duSerialIntervalCOVID192020; @liEarlyTransmissionDynamics2020; @nishiuraSerialIntervalNovel2020; @tindaleEvidenceTransmissionCOVID192020; @xiaTransmissionCoronaVirus2020; @xuHouseholdTransmissionsSARSCoV22020; @youEstimationTimevaryingReproduction2020; @zhangEvolvingEpidemiologyTransmission2020; @zhaoPreliminaryEstimationBasic2020a]. The mean and standard deviation of parameterised distributions were extracted and are presented in `r ref$tab("lit-review")`. The estimates of the mean range from 3.95 days to 7.5 days. The majority of studies provided their results as gamma distributions defined by mean and standard deviation. Some studies, particularly Xu et al.[@xuHouseholdTransmissionsSARSCoV22020] noted that the serial interval was not infrequently negative.

`r cap$tab("lit-review","Sources of serial interval estimates from a literature search")`

```{r table1}

# serialIntervals2 = serialIntervals %>% bind_rows(tibble(
#   mean_si_estimate = calcGammaMean("est")[["mean"]],
#   mean_si_estimate_low_ci = calcGammaMean("CIlow")[["mean"]],
#   mean_si_estimate_high_ci = calcGammaMean("CIhigh")[["mean"]],
#   std_si_estimate = calcGammaMean("est")[["sd"]],
#   std_si_estimate_low_ci = calcGammaMean("CIlow")[["sd"]],
#   std_si_estimate_high_ci = calcGammaMean("CIhigh")[["sd"]],
#   sample_size = 50L,
#   population = "UK",
#   assumed_distribution = "gamma",
#   estimate_type = "serial interval",
#   source = "Current analysis",
#   note = "none"
# ))

defaultSI = SerialIntervalProvider$default(dpc)

SerialIntervalProvider$printSerialIntervalSources() %>%
  group_by(`Reference`) %>% 
  standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/Table1_serialIntervals", defaultFontSize = 8, colWidths=c(4.5,2,1.5,1.5,1,1,1))

```



The random effects meta-analysis on the subset of studies which reported gamma distributions, resulted in an overall estimate of the serial interval that follows a `r si2$printSerialInterval()` (for more details see `r ref$sfig("meta-analysis")`). 


In `r ref$fig("si-estimates")`, panel A we present the results of the re-sampled serial interval estimate. The histogram shows the empirical distribution of the combination of all the studies, reinforcing the finding of a substantial proportion of the serial interval being negative. For the gamma and negative binomial distribution fit the data is truncated at zero, and the full data used for the normal distribution, resulting with mean values of 5.59 (gamma), 5.53 (negative binomial) or 4.88 (normal) days. Full detail of the parameterisation of this is available in `r ref$stab("dfit-serial-interval-resample")`.

In `r ref$fig("si-estimates")` panel B we present the distribution of the 50 linked cases in the FF100 data set for which onset dates are available for both infector and infectee. As with the resampled data the parameterised versions of this are based on truncated data for negative binomial and gamma distributions, and hence shows a poorer fit against the whole distribution (full detail of the parameterisation of this is available in `r ref$stab("dfit-serial-interval-ff100")`). Supporting the observations of Xu et al.[@xuHouseholdTransmissionsSARSCoV22020], the FF100 data shows evidence of negative serial intervals. The mean of the serial interval from FF100 data was 3.54 days when parameterised with a gamma distribution and data truncated to exclude negative serial intervals, and 2.09 days when a normal distribution used, with no truncation. This is on the lower end of the values reported in the literature.


```{r}

label = distLabel(si3$dfit)
panel1 = si3$dfit$plot(xlim=c(-7,10))+#guides(fill="none",colour="none")+
     standardPrintOutput::cornerAnnotation(label,pos="NW")+standardPrintOutput::narrower()+
     xlab("days")

label2 = label = distLabel(si1$dfit)
panel2 = si1$dfit$plot(xlim = c(-7,21))+#guides(fill="none",colour="none")+
     standardPrintOutput::cornerAnnotation(label2)+standardPrintOutput::narrower()+
     xlab("days")


fig1 = (panel2|panel1)/patchwork::guide_area() + patchwork::plot_annotation(tag_levels = "A")+plot_layout(heights = c(1,0.1),guides="collect")
fig1 %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/serial-interval/Fig2_SerialIntervalEstimates")


si1$getBasicConfig(quick = FALSE)$si_sample %>% write.table("~/Git/uk-covid-datatools/vignettes/serial-interval/resampled-truncated-empirical-si-sample.txt")
```

`r cap$fig("si-estimates","Panel A - Days between infected infectee disease onset based on a resampling of published estimates from the literature and Panel B - Estimates of serial interval from FF100 data. The histogram in panel A shows the combined density of all sets of samples within the original research.")`
 

As noted in the methods, the re-sampling process allows us to estimate the serial interval as an empirical distribution. Within EpiEstim, our chosen framework for estimating *R~t~* however the use of negative serial intervals are not supported as a proxy for the infectivity profile. Pragmatically we therefore decided to truncate the empirical distribution at zero for use in EpiEstim. Our adopted estimate therefore has a mode of 4.8 days, in line with the normal distribution parameterisation but is a `r si1$printSerialInterval()`, which is more in line with the gamma distribution parameterisation.

## Incubation period estimation

`r ref$fig("incub-period")` and `r ref$tab("incub-period-gof")` show the results of estimating a parametric probability distribution to data from FF100 and data from the Open COVID-19 Data Working Group. Histograms of the data are not shown as it is interval censored, which is not straightforward to represent graphically. There are only a small number of records from the FF100 data which suggest the mean of the incubation period is between 1.8 and 2 days. The data from the Open COVID-19 Data Working Group suggests the incubation period is longer with a mean around 5.5 days depending on the distribution chosen, and this agrees better with other estimates in the literature[@lauerIncubationPeriodCoronavirus2020; @zhangMetaanalysisSeveralEpidemic2020;@sancheHighContagiousnessRapid]. The best fit to the Open COVID-19 data is obtained with a log normal distribution as seen in `r ref$tab("incub-period-gof")` (Full details of the fitting parameters are in `r ref$stab("incub-period-detail")`). 

```{r fig5}

latestDataFile = dpc$datasets$downloadAndUntar(id = "BE-OUTBREAK-PREPARED",url = "https://github.com/beoutbreakprepared/nCoV2019/raw/master/latest_data/latestdata.tar.gz", pattern = "latestdata.csv")
latestdata <- readr::read_csv(
  latestDataFile,
  col_types = readr::cols(.default = readr::col_character())
)

latestdata2 = latestdata %>% filter(!is.na(date_onset_symptoms) & !is.na(travel_history_dates))
latestdata2 = latestdata2 %>% select(date_onset_symptoms,travel_history_dates) %>% 
  mutate(
    date_onset_symptoms = as.Date(date_onset_symptoms, "%d.%m.%Y"),
  ) %>% separate(
    travel_history_dates, into=c("travel.from","travel.to"),sep="[^0-9\\.]+", fill="left"
  ) %>%
  mutate(
    travel.from = as.Date(travel.from, "%d.%m.%Y"),
    travel.to = as.Date(travel.to, "%d.%m.%Y")
  ) %>%
  mutate(
    travel.from = ifelse(is.na(travel.from),travel.to,travel.from),
    left = as.integer(date_onset_symptoms - travel.to),
    right = as.integer(date_onset_symptoms - travel.from)
  )

bopFit = DistributionFit$new(c("gamma","lnorm","nbinom","weibull"))
bopFit$models$weibull$lower$shape = 1
bopFit$models$weibull$start$shape = 1.1
bopFit$models$gamma$lower$shape = 1
bopFit$models$gamma$start$shape = 1.1
bopFit$fromCensoredData(latestdata2,lowerValueExpr = left, upperValueExpr = right,truncate = TRUE,bootstraps = 200)



# and from FF100

ff100 = dpc$spim$getFF100()

censIncub = ff100 %>% filter(!is.na(date_exposure_first)) %>% select(date_exposure_first,date_exposure_last,date_onset) %>%
  mutate(
    right = as.numeric(date_onset-date_exposure_first), 
    left = as.numeric(date_onset-date_exposure_last)) %>%
  mutate( 
    left = ifelse(left<=0,NA_integer_,left)
  ) %>% filter( right > 0)

incubFF100Fit = DistributionFit$new(distributions = c("weibull","lnorm","gamma","nbinom"))
incubFF100Fit$models$weibull$lower$shape = 1
incubFF100Fit$models$weibull$start$shape = 1.1
incubFF100Fit$models$gamma$lower$shape = 1
incubFF100Fit$models$gamma$start$shape = 1.1
incubFF100Fit$fromCensoredData(censIncub,lowerValueExpr = left,upperValueExpr = right,truncate = TRUE, bootstraps = 200)
#incubFF100Fit$plot(xlim=c(0,7))
```

```{r}

# lauerFit$printDistributionDetail()
label3 = distLabel(bopFit)
fig5b = bopFit$plot(xlim = c(0,15))+
     standardPrintOutput::cornerAnnotation(label3)+
     xlab("days")+ylab("density")+standardPrintOutput::narrower()

label4 = distLabel(incubFF100Fit)
fig5a = incubFF100Fit$plot(xlim = c(0,15))+
     standardPrintOutput::cornerAnnotation(label4)+
     xlab("days")+ylab("density")+standardPrintOutput::narrower()+
    coord_cartesian(ylim=c(0,0.75))
#fig6 %>% standardPrintOutput::saveSixthPageFigure("~/Dropbox/covid19/serial-interval/Fig6_ff100Incubation")

fig5 = (fig5a|fig5b)/patchwork::guide_area()+patchwork::plot_annotation(tag_levels = "A")+patchwork::plot_layout(guides = "collect", heights=c(10,1))

fig5 %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/serial-interval/Fig3_IncubationPeriodsBeOutbreakPreparedAndFF100")
```

`r cap$fig("incub-period","Incubation period distributions reconstructed from Open COVID-19 Data Working Group and from FF100 data. Histogram data is not shown as the interval censored data cannot be plotted in this form")`


`r cap$tab("incub-period-gof","Goodness of fit statistics for incubation period distributions reconstructed from Open COVID-19 Data Working Group and from FF100 data")`

```{r}
bind_rows(
  incubFF100Fit$printDistributionDetail() %>% mutate(source = "FF100"),
  bopFit$printDistributionDetail() %>% mutate(source = "Open COVID-19 Data Working Group")
) %>% ungroup() %>% select(Source = source, N = n, AIC = aic, BIC = bic, `Log-likelihood`=loglik, Distribution) %>% distinct() %>% 
  group_by(Source,N) %>% arrange(AIC, Distribution) %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/Table_IncubationPeriodFitQuality",colWidths = c(2,1,2,2,2,2),defaultFontSize = 8)
```

## Generation interval estimation



```{r}
incubFit = bopFit$clone()
incubFit$filterModels(aic == min(aic))
incubFit$bootstraps = incubFit$bootstraps %>% filter(bootstrapNumber <= 100)

# generate a set of samples from best fitting incubation period distribution
# get samples from incubation and split into 2 groups - one for index patient and one for affected patient
incubFit$generateSamples(sampleExpr = 2000,seed = 101)
incubSamples = incubFit$samples %>%
  mutate(sampleCat = (sampleNumber-1) %/% 1000 + 1, sampleNumber = ((sampleNumber-1) %% 1000)+1) %>%
  pivot_wider(names_from = sampleCat, values_from = value, names_prefix = "incub")
incubDist = incubSamples %>% mutate(delayOffset = incub2, incubError = incub1-incub2, transition = "infection to onset", from="infection", to = "onset")


actualSI = si1$bootstrapSamples %>% select(bootstrapNumber,value)
errorGItoSI = incubDist %>% filter(incub1 < 14 & incub2 < 14)  %>% select(bootstrapNumber, error=incubError) 

simSi = function(shape, rate, errors) {
  N = length(errors)
  genSim = rgamma(N*100,shape,rate = rate)+rep(errors,100)
  return(genSim)
}

errorFunction = function(simulated, actuals) {
  return(
    #abs(mean(actuals)-mean(simulated))+
    abs(IQR(actuals)-IQR(simulated))
  )
}

minimise = function(shape, rate, errors, actuals) {
  simulated = simSi(shape,rate,errors)
  out = errorFunction(simulated, actuals)
  return(out)
}

estimateParams = function(errors,actuals) {

  mu = mean(actuals)
  search = function(sdLim=c(mu/5,mu), grid = NULL) {
    #browser()
    sdWidth=(sdLim[2]-sdLim[1])/10
    if(sdWidth<0.00001) return(grid)
    for(sd in seq(sdLim[1]+sdWidth,sdLim[2]-sdWidth,length.out = 5)) {
      shape = mu^2/sd^2
      rate = mu/sd^2
      grid = grid %>% bind_rows(
        tibble(
          mean = mu,
          sd = sd,
          shape=shape,
          rate=rate,
          error=minimise(shape,rate,errors,actuals)
        )
      )
    }
    gridmin = grid %>% filter(error==min(error))
    grid = gridmin %>% group_modify(function(d,g,..) {
      return(search(
        d$sd+c(-1,1)*sdWidth,
        grid
      ))
    })
    return(grid %>% distinct())
  }

  return(search() %>% filter(error == min(error)))
}


expt = actualSI %>% group_by(bootstrapNumber) %>% nest(actual=value) %>% inner_join(
  errorGItoSI %>% group_by(bootstrapNumber) %>% nest(error=error),
  by="bootstrapNumber")



genIntTmp = dpc$getHashCached(object = expt,operation = "GENERATION-INTERVAL-OPTIM",orElse = function(...) {
  genInt = expt %>% group_by(bootstrapNumber) %>% group_modify(function(d,g,...) {
    actuals=d$actual[[1]]$value
    errors = d$error[[1]]$error
    #initMean = mean(actuals)
    #initSd = sd(actuals)
    message(".",appendLF = FALSE)
    #out = optim(par = c(initMean,initSd), fn = minimise, lower = c(0,0), method = "L-BFGS-B", errors=errors, actuals=actuals)
    out = estimateParams(errors=errors, actuals=actuals)
    #return(tibble(mean = out$par[1],sd = out$par[2], err = out$value))
    return(out)
  })
  #genInt = genInt %>% mutate(shape = mean^2/sd^2, rate = mean/sd^2)
  generationIntervalSimulation = genInt %>% select(bootstrapNumber,shape,rate) %>% pivot_longer(cols = c(shape,rate), names_to = "param", values_to = "value") %>% mutate(dist="gamma")
  usethis::use_data(generationIntervalSimulation, overwrite = TRUE)
  return(generationIntervalSimulation)
})

si4 = SerialIntervalProvider$generationInterval(dpc,bootstrapsDf = genIntTmp)

# export gen int sample probability distributions
tmp = si4$getCustomConfigs(quick=FALSE)[[1]][[1]]$si_sample
tmp %>% write.table("~/Git/uk-covid-datatools/vignettes/serial-interval/generation-interval-fitted-si-sample.txt")

# export distribution parameters
si4$dfit$bootstraps %>% pivot_wider(names_from = "param",values_from = "value") %>% 
  mutate(mean = shape/rate, sd = sqrt(shape/(rate^2))) %>% write_csv("~/Git/uk-covid-datatools/vignettes/serial-interval/generation-interval-fitted-parameters.csv")

```

The generation interval is then inferred from the incubation period and empirical serial interval distribution prior to truncation. Our best estimate for this is a `r si4$printSerialInterval()`, as shown in `r ref$fig("generation-interval")`. The mean of 4.8 days is identical to that of the empirical serial interval distribution prior to truncation in panel B, `r ref$fig("si-estimates")` as a result of the constraints imposed during fitting. The standard deviation of our generation interval estimate is 1.72. This is within the confidence limits of estimates from the literature from both China (0.74 - 2.97) and Singapore (0.91 - 3.93)[@ganyaniEstimatingGenerationInterval2020a].

```{r}
label5 = si4$dfit$printDistributionDetail() %>% ungroup() %>% summarise(label = paste0(param,": ",`Mean Â± SD (95% CI)`,collapse = "\n"))
# label5 = distLabel(si4$dfit)
fig6 = si4$dfit$plot(c(0,15))+xlab("Generation interval (days)")+ylab("density")+guides(fill="none",colour="none")+standardPrintOutput::cornerAnnotation(label5)

fig6 %>% standardPrintOutput::saveSixthPageFigure("~/Dropbox/covid19/serial-interval/Fig4_GenerationInterval")

# https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html
# https://magesblog.com/post/2016-09-27-fitting-distribution-in-stan-from/

```

`r cap$fig("generation-interval", "Estimated generation interval distributions, from resampled serial intervals as predictor, and estimated serial intervals from incubation period combined with samples from a generation interval assumed as a gamma distributed quantity.")`

## Impact of using the serial interval on estimation of *R~t~*

With our 3 estimates of the serial interval and 1 generation interval and observed COVID-19 case counts, we investigate the impact on the estimates of *R~t~*, of using these estimates as a proxy for the infectivity profile. This uses data at 4 time points on an epidemic curve from the first wave of the COVID-19 outbreak in England, shown in `r ref$fig("epi-curve")`, which are 19th March, 12th April, 12th May and 23rd June, corresponding to the ascending, peak, early descending and late descending phases respectively.

```{r fig3}
siDates = tibble(
  `Start date` = as.Date(c("2020-03-19","2020-04-12","2020-05-12","2020-06-23")),
  `End date` = NA,
  `Label` = c("ascending","peak","early descending","late descending")
)

ukts = dpc$datasets$getPHEApi(areaName = "england") %>% filter(date >= as.Date("2020-03-01") & date<=as.Date("2020-06-30")) 

ukts = ukts %>% bind_rows(
  ukts %>% group_by(statistic) %>% filter(date == min(date)) %>% select(-date) %>% mutate(value=0) %>% left_join(
    tibble(date = as.Date(as.Date("2020-02-16"):as.Date("2020-02-29"),"1970-01-01")), by=character()
  )
)

ukts = ukts %>% 
  tsp$imputeAndWeeklyAverage() #%>%
  #tsp$logIncidenceStats(smoothingWindow = 14, growthRateWindow = 14)

fig3 = ukts %>% tsp$plotIncidenceQuantiles(colour = statistic, events = siDates, labelSize=9) +scale_color_brewer(palette="Dark2")  #+ geom_vline(data = tibble(date=ends), aes(xintercept=date), colour="black")

fig3 %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/serial-interval/Fig5_EpidemicCurve")

#ukts %>% tsp$plotRt(colour = statistic)


```

`r cap$fig("epi-curve","Epidemic curve for cases, deaths and hospital admissions are used for analysis in this paper. Dashed vertical lines show dates at which we conduct our analysis, chosen to represent the ascending, peak, early and late descending phases of cases during the first wave in the UK.")`

```{r cache=TRUE}

meanlim=c(2,8)
sdlim=c(1,6)

ukcases = ukts %>% tsp$logIncidenceStats() %>% filter(statistic=="case")

siImpact = dpc$datasets$getHashCached(object=ukcases, params = list(siDates, meanlim,sdlim), operation = "SERIAL-INTERVAL-IMPACT", orElse = function(...) {
  # perform the Rt estimate for the various mean sd combinations
  I = ukcases %>% pull(Est.value)
  dates = ukcases %>% pull(date)
  
  out = NULL
  
  for(endDate in siDates$`Start date`) {
  
    event = siDates$Label[siDates$`Start date` == endDate]
    start = match(endDate,dates)-3
    end = match(endDate,dates)+3
    
    for (siMean in seq(meanlim[1],meanlim[2],length.out = 40)) {
      #siMean = 4
      for (siSd in seq(sdlim[1],sdlim[2],length.out = 40)) {
      #siSd = 4  
        
        cfg_tmp = EpiEstim::make_config(mean_si = siMean, std_si = siSd, t_end = end, t_start = start, method="parametric_si")
        rEst = EpiEstim::estimate_R(I, method="parametric_si", config = cfg_tmp)
        out = bind_rows(
          out,
          tibble(
            startDate = as.Date(endDate-7,"1970-01-01"),
            endDate = as.Date(endDate,"1970-01-01"),
            label = event,
            siMean = siMean,
            siSd = siSd,
            medianR = rEst$R$`Median(R)`
          )                    
        )
      }
    }
    
  }
  return(out)
})

#serialIntervals = readr::read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vRdVV2wm6CcqqLAGymOLGrb8JXSe5muEOotE7Emq9GHUXJ1Fu2Euku9d2LhIIK5ZvrnGsinH11ejnUt/pub?gid=0&single=true&output=csv")

siFits = bind_rows(
  tibble(
    name = names(si1$getSummary()),
    value = as.numeric(unlist(si1$getSummary()))
  ) %>% pivot_wider(names_from = name, values_from = value) %>% mutate(name = "resample"),
  tibble(
    name = names(si3$getSummary()),
    value = as.numeric(unlist(si3$getSummary()))
  ) %>% pivot_wider(names_from = name, values_from = value) %>% mutate(name = "ff100"),
  tibble(
    name = names(si2$getSummary()),
    value = as.numeric(unlist(si2$getSummary()))
  ) %>% pivot_wider(names_from = name, values_from = value) %>% mutate(name = "random effects"),
  tibble(
    name = names(si4$getSummary()),
    value = as.numeric(unlist(si4$getSummary()))
  ) %>% pivot_wider(names_from = name, values_from = value) %>% mutate(name = "generation")
)

tmp = siImpact %>% left_join(siFits, by=character()) %>% group_by(name) %>% filter(abs(meanOfMean-siMean) == min(abs(meanOfMean-siMean)) & abs(meanOfSd-siSd) == min(abs(meanOfSd-siSd)))
tmp2 = tmp %>% ungroup() %>% group_by(startDate, label) %>% summarise(min = min(medianR), max= max(medianR)) %>% mutate(delta = max-min, percent = delta*2/(max+min)*100) %>%
  mutate(desc = sprintf("%s - %1.0f%% variation (*R~t~*: %1.2f to %1.2f)",label,percent,min,max))
summaryImpact = paste0(tmp2$desc,collapse = "; ")

```

At each of these 4 time points `r ref$fig("si-impact")` shows the estimated *R~t~* under the range of different assumptions about the mean and standard deviation of the infectivity profile, modeled as a gamma distribution. In the top left, bottom left and bottom right panels the effect of increasing the mean of the infectivity profile is to push the resulting estimate of *R~t~* away from the critical value of 1 at which the epidemic is growing. In the top right panel, at the peak, the mean of the infectivity profile has a less clear-cut effect. The impact of changes to standard deviation is likewise varied. In the top left, bottom left and bottom right panels during ascending and descending phases there is relatively little impact of changing the standard deviation of the infectivity profile on the estimates of *R~t~*, and any small changes that do occur depend on the shape of the preceding epidemic curve. At the peak however, in the top right panel, the wider the standard deviation the more historical information influences the estimation of *R~t~* and this acts to delay the estimated transition from positive to negative growth. The overall result of this is that estimates of the infectivity profile with a high standard deviation will predict *R~t~* crossing 1 later than estimates based on an infectivity profile with a low standard deviation, but the point of crossing 1 is relatively insensitive to the value of the mean of the infectivity profile.

When we consider using the various estimates of the serial interval or generation interval, as a proxy for the infectivity profile, on the resulting estimates of *R~t~* we can see from the coloured crosses on `r ref$fig("si-impact")` representing the different estimates of serial or generation interval, that in the situations of dynamic change such as the ascending phase the variability may have quite a large impact on subsequent estimates of *R~t~* but at other times the impact is much smaller [`r summaryImpact`].

```{r fig4}

plotSiVariability = function(d, xlim = meanlim, ylim = sdlim) { #, contours) {
  data = siImpact %>% filter(label == d & siMean >= xlim[1] & siMean <= xlim[2] & siSd >= ylim[1] & siSd <= ylim[2]) %>% mutate(facetLabel = paste0(d," - ",siDates %>% filter(Label==d) %>% pull(`Start date`)))
  contours = round(quantile(data$medianR,seq(0.1,0.9,0.1)),digits = 2)
  # seq(
  #   ceiling(min(data$medianR)*10)/10,
  #   floor(max(data$medianR)*10)/10,
  #   length.out = 11
  # )
  ggplot(data, aes(x=siMean, y=siSd, z=medianR, fill=1/medianR))+ #geom_tile()+
    metR::geom_contour2(colour="black", breaks=contours,alpha=0.8)+
    # scale_fill_gradient2(high="cyan",mid="white",low="orange", midpoint=1, limits=c(1/4,1/0.6), guide="none", oob=scales::squish)+
    
    # geom_linerange(data=serialIntervals, aes(
    #   xmin=mean_si_estimate_low_ci,xmax=mean_si_estimate_high_ci,
    #   y=std_si_estimate),inherit.aes = FALSE,colour="blue", alpha=0.8)+
    # geom_linerange(data=serialIntervals,aes(
    #   x=mean_si_estimate,ymin=std_si_estimate_low_ci,ymax=std_si_estimate_high_ci),inherit.aes = FALSE,colour="blue", alpha=0.8)+
    geom_point(data=serialIntervals,aes(x=mean_si_estimate,y=std_si_estimate, size=sample_size),inherit.aes = FALSE,colour="blue",show.legend = FALSE)+
    
    geom_linerange(data=siFits,aes(
      xmin=minOfMean, xmax=maxOfMean,
      y=meanOfSd,colour=name),inherit.aes = FALSE, size=1, alpha=0.8)+
    geom_linerange(data=siFits,aes(
      x=meanOfMean,
      ymin=minOfSd, ymax=maxOfSd,colour=name),inherit.aes = FALSE, size=1, alpha=0.8)+
    geom_point(data=siFits,aes(x=meanOfMean,y=meanOfSd,colour=name),inherit.aes = FALSE, size=2)+
    
    
    metR::geom_text_contour(colour="black", breaks=contours,stroke=0.3,alpha=0.8, size=standardPrintOutput::labelInPoints(12))+
    
    coord_cartesian(xlim,ylim)+
    scale_size(range=c(0.1,2),name = "Samples")+
    scale_color_brewer(palette = "Dark2")+
    labs(x="SI Mean",y="SI Std Dev")+
    facet_wrap(vars(facetLabel))+
    theme(plot.margin = unit(c(2,2,2,2),"pt"))+standardPrintOutput::narrower()#+
    # theme(
    #     plot.subtitle = element_text(margin = margin(.05, 0, .05, 0, "cm"), size=8),
    #     plot.subtitle.background = element_rect(fill = "#F8F8F8")
    # )
}

plts = sapply(siDates$Label, FUN = plotSiVariability, simplify = FALSE)
library(grid)
p1 = plts[[1]]+hideX()
p2 = plts[[2]]+hideX()+hideY()
p3 = plts[[3]]
p4 = plts[[4]]+hideY()

(((p1|p2)/(p3|p4)/patchwork::guide_area()) +patchwork::plot_layout(guides = "collect",height=c(5,5,1)))  %>% saveHalfPageFigure("~/Dropbox/covid19/serial-interval/Fig6_EstimatesOfRtForSIAssumptions")

```

`r cap$fig("si-impact","Time varying reproduction number given various assumptions on the serial interval mean and standard deviation. The blue points show the central estimate of serial intervals from the literature, whereas the coloured error bars show the mean and standard deviation of the 3 serial interval (green, violet, magenta) and 1 generation interval (orange) estimates presented in this paper. Contours show the *R~t~* estimate for that combination of mean and standard deviation serial interval. The four panels represent the 4 different time points investigated.")`

## Time delays from infection to case identification, admission, and death

```{r}
CHESS = dpc$spim$getCHESS()
CHESSClean = CHESS %>% chp$chessAdmissionSubset()
#plots=list()
#tables=list()

```

```{r}

onsetToTest = CHESSClean %>% 
    filter(age>10 & !is.na(estimateddateonset) & !is.na(infectionswabdate)) %>% 
    mutate(
      transition = "onset to test",
      time = as.integer(infectionswabdate - estimateddateonset)+0.01
    ) %>% select(caseid,transition,time) %>% filter(time < 28 & time > -14) %>% group_by(transition)

onsetToAdmission = CHESSClean %>% 
  # Onset ot admission
    filter(age>10 & !is.na(estimateddateonset) & !is.na(hospitaladmissiondate)) %>% 
    mutate(
      transition = "onset to admission",
      time = as.integer(hospitaladmissiondate - estimateddateonset)+0.01
    ) %>% select(caseid,transition,time) %>% filter(time < 100 & time > 0) %>% group_by(transition)

onsetToDeath = CHESSClean %>% 
  filter(age>10 & !is.na(estimateddateonset) & !is.na(finaloutcomedate) & finaloutcome=="Death") %>% 
    mutate(
      transition = "onset to death",
      time = as.integer(finaloutcomedate - estimateddateonset)
    ) %>% select(caseid,transition,time) %>% filter(time < 100 & time > 0) %>% group_by(transition)

rawOnset = bind_rows(
  onsetToTest, onsetToAdmission, onsetToDeath
) %>% group_by(transition)


onsetFit = DistributionFit$new(distributions = c("lnorm","gamma","weibull","nbinom"))$fromUncensoredData(rawOnset, valueExpr = time, truncate = TRUE, bootstraps = 100)
onsetPlot = onsetFit$plot(xlim = c(-2,20))+xlab("onset to observation (days)")+standardPrintOutput::narrowAndTall()

#plots$onsetToTest = onsetToTestFit$plot(xlim = c(-2,20))+xlab("time delay")
#tables$onsetToTest = onsetToTestFit$printDistributionDetail()
```

In `r ref$fig("chess-delay")` we show probability distributions fitted to data from the CHESS data set which define times from symptom onset (Panel A) to case identification (*T~onsetâ†’case~*), admission (*T~onsetâ†’admission~*), or death (*T~onsetâ†’death~*). Symptom onset to test (case identification) can be a negative quantity if a swab is taken during disease screening and the patient is pre-symptomatic. In this data the time point that defines the time of test is the date when the specimen is taken, which will subsequently be tested positive for SARS-CoV-2, so does not include sample processing delays. However in this hospital based data source of admitted patients the onset data was collected retrospectively. We also note peaks at 1 day, 1 week, 2 weeks, and so on which suggests approximation on data entry, and there may well be biases in the data collection. 

It is more obvious from the clinical course of COVID-19 that admission should occur after disease onset. In the data, a large number of cases are reported to have symptom onset on day of admission. This is potentially a reporting artifact as in the absence of certain knowledge about onset, it is possible that the day of admission may have been captured instead, and we again see the peaks at 1 week, 2 weeks, and so on, suggesting approximations in data entry. 

The time between onset of symptoms and death can also be assumed as a positive quantity given this is based on an in hospital cohort. This distribution shows a large tail, and some patients in that tail were noted to be admitted many months before the appearance of COVID-19. These patients likely represent hospital acquired cases in chronically unwell patients. The extreme outlying values (with delay from admission to death greater than 100 days, or with an admission before 1st Jan 2020) were removed as they prevented sensible estimation of the rest of the distribution.

By combining the incubation period distribution in panel B in `r ref$fig("incub-period")` with the time delay distributions in panel A of `r ref$fig("chess-delay")`, we can obtain probability distributions from infection to observation, and these are shown in panel B for the 3 observations of test (case identification), admission and death. These distributions provide us with a means of estimating a time series of infection from observed case counts, admissions and deaths. As above full details of their parameterisations are available in `r ref$stab("chess-delay-params")`. The mean time from infection to the various time points described in the timeline in `r ref$fig("infection-timeline")` are presented in `r ref$tab("rt-offset")`. The infection to onset is the incubation period, with a mean of 5.5 days. On average approximately 8 days pass from infection to diagnosis, a subsequent 1 days until admission, and a further 7-8 days until death, however it also shows considerable variation in these delays, exemplified by the 95% quantiles for the time from infection to death estimated as ranging from 3.6 days to nearly 50 days.

```{r}

# generate a observation-observation delay error function
# this is resampling raw CHESS delays to have a fixed number of bootstraps of given length & generating a delay distribution by looking at diffence between random resamples
rawDelay = rawOnset %>% mutate(transition = stringr::str_replace(transition,"onset to ","infection to ")) %>% group_by(transition) %>% group_modify(function(d,g,...) {
  boot = 1:100
  tmp = lapply(boot, function(b) tibble(
    bootstrapNumber = b,
    sampleNumber = 1:1000,
    delay1 = sample(d$time,size=1000),
    delay2 = sample(d$time,size=1000),
    delayOffset = delay2-delay1
  ))
  return(bind_rows(tmp))
})

# combine incubation period + observation delay of infectee period to get time correction
combinedDelay = rawDelay %>% 
  left_join(incubDist, by=c("bootstrapNumber","sampleNumber"), suffix=c("",".incub")) 

infectionToObservationDelay = combinedDelay %>% 
  mutate(timeDelay = incub2+delay2) %>% ungroup() %>%
  select(transition,timeDelay,bootstrapNumber,sampleNumber)

infectionToObservationFit = DistributionFit$new(distributions = c("lnorm","gamma","nbinom","weibull"))
infectionToObservationFit$fromBootstrappedData(infectionToObservationDelay %>% select(-sampleNumber) %>% group_by(transition), valueExpr = timeDelay)

infectionPlot = (infectionToObservationFit$plot(c(-2,20))+standardPrintOutput::narrowAndTall()+xlab("infection to observation (days)"))

chessPlot = onsetPlot + infectionPlot +patchwork::guide_area() + patchwork::plot_annotation(tag_levels = "A") + patchwork::plot_layout(ncol=1, guides="collect",heights = c(5,5,1))
chessPlot %>% saveHalfPageFigure("~/Dropbox/covid19/serial-interval/Fig7_ChessTimeDelayDistributions")

```

`r cap$fig("chess-delay","Panel A: Time delay distributions from symptom onset to test (diagnosis or case identification), admission or death, estimated from CHESS data set, plus in Panel B estimated delays from infection to observation, and can be negative in certain cases. based on incubation period and observation delay. These  can be used for deconvolution.")`

`r cap$tab("rt-offset","Estimated time delays between infection and various observations over the course of an infection, based on the combination of incubation period and symptom onset to observation delay")`

```{r}
timeCorrection2 = infectionToObservationDelay %>%
  mutate(to = stringr::str_remove(transition,"infection to ")) %>%
  group_by(to) %>%
  summarise(
    mean = mean(timeDelay),
    sd = sd(timeDelay),
    lower = quantile(timeDelay,0.025),
    upper = quantile(timeDelay,0.975),
  ) %>% 
  bind_rows(
    incubDist %>% group_by(to) %>% summarise(
      mean = mean(delayOffset),
      sd = sd(delayOffset),
      lower = quantile(delayOffset,0.025),
      upper = quantile(delayOffset,0.975),
    )
  ) 

timeCorrection2 %>% write.csv(file=paste0(dpc$directory,"/","TIME-CORRECTION.csv"))

ukCovidObservationDelays = timeCorrection2 %>%
  mutate(
    `Mean delay (days)` = sprintf("%1.2f",mean),
    `SD (days)` = sprintf("%1.2f",sd),
    `95% quantiles (days)` = sprintf("%1.2f; %1.2f",lower, upper)
  )
usethis::use_data(ukCovidObservationDelays, overwrite = TRUE)

ukCovidObservationDelays %>% arrange(mean) %>% select(c(-mean,-sd,-lower,-upper)) %>%
  rename(Observation=to) %>%
  standardPrintOutput::saveTable(filename="~/Dropbox/covid19/serial-interval/Table2_TimeCorrectionsFromIncubationPeriodAndDelayToObservation", defaultFontSize = 8)

```

## Estimation of *R~t~*


```{r}
# https://www.rdocumentation.org/packages/surveillance/versions/1.12.1/topics/backprojNP
# http://freerangestats.info/blog/2020/07/18/victoria-r-convolution

convolutionsFit = infectionToObservationFit$clone()
convolutionsFit$filterModels(aic == min(aic))
pmfs = convolutionsFit$discreteProbabilities(q = 0:30)
pmfs = pmfs %>% mutate(statistic = case_when(
  transition == "infection to test" ~ "case",
  transition == "infection to admission" ~ "hospital admission",
  transition == "infection to death" ~ "death",
  TRUE ~ NA_character_
))

ukts2 = ukts %>% group_by(statistic) %>% group_modify(function(d,g,...) {

  caseSts = surveillance::sts(
    observed = d %>% pull(Imputed.value) ,
    epoch = d %>% pull(date)
  )
  
  casePmf = pmfs %>% filter(statistic == g$statistic) %>% pull(Mean.discreteProbability)
  
  # Back-propagate to get an estimate of the original, assuming our observations
  # are a Poisson process
  
  bpnp.control <- list(k=0,eps=rep(0.005,2),iter.max=rep(250,2),B=-1) #,verbose=TRUE)
  bpnp.control2 <- modifyList(bpnp.control, list(hookFun=NULL,k=2,B=100,eq3a.method="C"))
  
  #Fast C version (use argument: eq3a.method="C")! 
  sts.bp <- surveillance::backprojNP(caseSts, incu.pmf=casePmf, control=bpnp.control2)
  
  d = d %>% mutate(
    Deconv.value = sapply(1:dim(sts.bp@lambda)[1], FUN=function(x) {mean(sts.bp@lambda[x,,])}),
    Deconv.0.975.value = sapply(1:dim(sts.bp@lambda)[1], FUN=function(x) {quantile(sts.bp@lambda[x,,],0.975)}),
    Deconv.0.025.value = sapply(1:dim(sts.bp@lambda)[1], FUN=function(x) {quantile(sts.bp@lambda[x,,],0.025)})
  )
  
  return(d)

})

# incidence = (ukts2 %>% tsp$plotIncidenceQuantiles(colour = statistic, dates = c("2020-03-01","2020-07-01"))) + 
#   geom_line(aes(y=Deconv.value,colour=statistic),linetype="21") + 
#   geom_ribbon(aes(ymin=Deconv.0.025.value,ymax=Deconv.0.975.value,group=statistic),fill="black",colour=NA,alpha=0.2) + 
#   guides(fill="none",colour="none")+
#   facet_wrap(vars(statistic))


ukts3 = ukts2 %>%
  tsp$logIncidenceStats(valueVar = Deconv.value, growthRateWindow = 14, smoothingWindow = 14) %>%
  tsp$estimateRtQuick(valueVar = Est.Deconv.value, serialIntervalProvider = SerialIntervalProvider$generationInterval(dpc), window = 7)
#ggplot(result,aes(x=date,fill=statistic,colour=statistic))+geom_point(aes(y=observed))+geom_line(aes(y=smoothed))+geom_line(aes(y=Deconv.value),linetype="dashed")+geom_ribbon(aes(ymin=lower,ymax=upper),fill="blue",colour=NA,alpha=0.2)

ukts4 = ukts2 %>%
  tsp$logIncidenceStats(valueVar = value, growthRateWindow = 14, smoothingWindow = 14) %>%
  tsp$estimateRtQuick(valueVar = Est.value, window = 7) %>%
  tsp$adjustRtDates(window = 0)

ukts6 = bind_rows(
  ukts2 %>% mutate(subgroup = "formal") %>% select(-value) %>% rename(value=Deconv.value),
  ukts %>% mutate(subgroup = "pragmatic")
)



ukts5 = bind_rows(
  ukts3 %>% mutate(subgroup = "formal"),
  ukts4 %>% mutate(subgroup = "pragmatic")
)

avRtError = ukts3 %>% ungroup() %>% select(date,statistic,deconv=`Median(R)`) %>% left_join(
  ukts4 %>% ungroup() %>% select(date,statistic,unadj=`Median(R)`), by=c("date","statistic")
) %>% mutate(percent = abs(2*(unadj-deconv)/(unadj+deconv))*100) %>% group_by(statistic) %>% 
  summarise(desc = sprintf("%1.2f%% (IQR %1.2f%%; %1.2f%%)", median(percent,na.rm=TRUE), quantile(percent,0.25,na.rm=TRUE), quantile(percent,0.75,na.rm=TRUE) )) %>%
  ungroup() %>% 
  mutate(out=sprintf("%s: sMAPE %s",statistic,desc)) %>%
  pull(out) %>% paste0(collapse="; ")

# rtDeconv = ukts3 %>% tsp$plotRt(colour = statistic,dates = c("2020-03-01","2020-07-01"))+facet_wrap(vars(statistic))+guides(fill="none",colour="none") #+scale_color_brewer(palette = "Dark2")
# rtRaw = ukts4 %>% tsp$plotRt(colour = statistic,dates = c("2020-03-01","2020-07-01"))+guides(fill="none",colour="none")+facet_wrap(vars(statistic))

incidence = ukts6 %>% tsp$plotIncidenceQuantiles(colour=subgroup,dates = c("2020-03-01","2020-07-01")) + facet_wrap(vars(statistic))+
  scale_color_brewer(palette = "Dark2")

rtComp = ukts5 %>% tsp$plotRt(colour = subgroup,dates = c("2020-03-01","2020-07-01"), rtlim=c(0.5,1.75))+facet_wrap(vars(statistic))+
  scale_color_brewer(palette = "Dark2")
  #scale_linetype_manual(values = c("21","solid"))+
  #standardPrintOutput::narrower()

deconvComparison = (incidence+standardPrintOutput::hideX()) + rtComp + patchwork::plot_annotation(tag_levels = "A") + patchwork::plot_layout(ncol = 1, guides = "collect", heights = c(5,5))

deconvComparison %>% standardPrintOutput::saveHalfPageFigure("~/Dropbox/covid19/serial-interval/Fig8_DeconvolutionVersusUndajusted")
# note the actual estimates are in x2@upperbound, which adds up to 39, same as the original y
```

`r cap$fig("deconv-compare","Panel A: The epidemic incidence curves in England for different observations (orange - formal) and inferred estimates of infection rates (green - pragmatic) based on a deconvolution of the time delay distributions. Panel B, the resulting *R~t~* values calculated either using infection rate estimates and generation interval (formal subgroup) or unadjusted incidence of observation, and serial interval (pragmatic). The *R~t~* estimated direct from observed incidence curves (pragmatic) have their dates adjusted by the mean delay estimate")`

With the estimates of delay from infection to observation we are able to use a non parametric back propagation as described in the methods to estimate a time series of infections as recommended by Gostic et al.[@gosticPracticalConsiderationsMeasuring2020a] when using EpiEstim. The results of the back propagation is shown in `r ref$fig("deconv-compare")`, panel A which depicts resulting point and smoothed infection curves associated with observation curves from England. The back projection results in a sharper and narrower epidemic curve than the observation it is derived from, and indicates additional structures (such as more pronounced fluctuations) which are not obvious from the underlying observations. Estimates of *R~t~* are shown in panel B based on de-convolved time series plus generation interval, versus raw observation counts, re-sampled serial interval estimates, with time adjustment on the resulting *R~t~* estimates to align *R~t~* estimate date to putative date of infection. We have not calculated confidence intervals for the estimates of *R~t~*. The estimates of *R~t~* differ from their mean (using symmetric mean absolute percentage error, sMAPE) by between 6% to 9% [`r avRtError`].

# Discussion

Our estimate of the serial interval from the FF100 UK data was found to be short, compared to international estimates. The FF100 data was collected in the early stage of the epidemic and is based mainly on household contacts of international travelers which may impart bias. As the participants in the study were put into self-isolation upon discovery observations the contact period tends to be shortened, leading to shorter serial interval estimates, and because of this we do not believe the estimate from the FF100 study to be specific to the UK but rather that the data set is not broadly representative of the UK population. 

In contrast, our literature search allowed us to estimate the serial interval from a much larger pooled sample drawn from multiple studies. A meta-analysis and a re-sampling study produced comparable results (meta-analysis: `r metaSIresult` and re-sampling: `r resampleSIresult`), but our re-sampling study more clearly showed the potential for the serial interval to be negative, due to the relatively long and variable incubation period of SARS-CoV-2. The negative values of the serial interval are theoretically problematic for their use as a proxy for the infectivity profile of SARS-CoV-2 in transmission modelling, which requires the infectee is infected after the infector. Pragmatically, as a way around this, we may truncate the re-sampled serial interval at zero, and use this as the infectivity profile, but as seen here this truncation increases the mean of the resulting serial interval distribution from 4.8 to 5.6 days.

An alternative to this is to use the generation interval as a proxy for the infectivity profile, but there are limited estimates of this quantity available in the literature[@ganyaniEstimatingGenerationInterval2020a]. Derivation of the generation interval from the serial interval is possible using knowledge of the incubation period. We again looked at the FF100 data for estimates of incubation period and again found that the UK data suggests a value which is shorter that international estimates, for the same reasons as mentioned above[@backerIncubationPeriod20192020; @duEstimatingDistributionCOVID192020; @lauerIncubationPeriodCoronavirus2020; @lintonIncubationPeriodOther2020; @xiaTransmissionCoronaVirus2020]. We cross referenced this with a second estimate derived from the Open COVID-19 Data Working Group data set[@kraemer2020epidemiological;@xuOpenAccessEpidemiological2020], which is based on a large international data set of people who tested positive for SAR-CoV-2 after travelling from areas with outbreaks. The resulting estimates of the mean incubation period of 5.5 days (log normally distributed) is much closer to previous estimates, and we expect to be less influenced by right censoring. Again, we regard the short incubation period calculated from the FF100 data set to be a feature of the data set rather than a UK specific finding. In fact the estimate based on the Open COVID-19 Data may in itself be an under-estimate as the majority of travel histories only include the return date of the visit in question and not the start date.

With incubation period and serial interval estimates, we derived an estimate for generation interval, assuming a gamma distribution. This was comparable to previous estimates in that although it is based on a different serial interval, and hence has a different mean, the standard deviation of our estimate is in accordance with that estimated by Ganyani et al.[@ganyaniEstimatingGenerationInterval2020a] Although the confidence intervals for the mean and standard deviation of the generation interval are comparable, the variation in the shape and rate parameters of the underlying gamma distribution are quite large. Care should be taken when generating bootstrap samples from the generation interval when it is specified as an uncertain gamma distribution parameterised with mean and standard deviation, as it is possible that some combinations of parameters produce unrealistic distributions, particularly when the standard deviation is small and the mean is large, which could in theory result in posterior estimates for *R~t~* being largely determined by the reciprocal of just a small number of observations.

Using estimates of the serial interval distribution and generation interval distribution as a proxy for infectivity profile, we investigated the resulting variation in the estimation of *R~t~* using incident cases in England and the forward equation approach. We found the bias between the smallest and largest of our estimates to be as high as 20-25% of the central estimate of *R~t~* when *R~t~* was high, but somewhat smaller when *R~t~* values were 1 or lower. Distributions with lower values of the mean tended to result in estimates of *R~t~* that were closer to one. This suggests that biases introduced by the use of different serial interval distributions should not influence the answer to the key question, "is *R~t~* greater or less than 1?" which defines whether the epidemic is expanding or contracting in size. It is also to be expected that the nature of change of *R~t~* over time is not affected by this bias, so an increasing value of *R~t~* will be increasing regardless of the infectivity profile that is used to estimate it.

Estimating *R~t~* is based on knowledge of the incidence of infections. This is not a quantity that is readily observed in the SARS-CoV-2 epidemic, and pragmatically use of observations including symptom onset, case identification, admission and death are expected to be used as proxy measures for infection[@coriNewFrameworkSoftware2013; @thompsonImprovedInferenceTimevarying2019a]. However, as pointed out elsewhere[@gosticPracticalConsiderationsMeasuring2020a] the variable time between infection and such observations causes the signal to be both delayed and blurred. By combining our estimates of incubation period with data from hospital admissions in the CHESS data set we were able to make estimates of the distribution of the delay from infection to observation for the UK. There are several caveats to this part of our analysis that must be kept in mind. The CHESS data set relies on a retrospective report of onset of symptoms, and this field is not recorded for all patients. We select only those patients who have reported an onset date and it may be the case that these patients represent a subgroup of patients whose symptom onset is significantly different from the average patient. The data collection around these dates was noted above to show patterns suggestive of rounding or approximation and this could also introduce some bias. The delay distributions are also unlikely to remain fixed during the outbreak, as we would hope to see the time from infection to case identification shorten during the epidemic, and the time from infection to death lengthen as treatment improves, and as the cohort of susceptible individuals changes. Our confidence in these distributions is therefore somewhat low, although we note acceptable agreement between our estimates produced using a mixture of international and UK data, and previously published estimates from different countries, most notably China[@jungRealTimeEstimation2020; @lintonIncubationPeriodOther2020; @sancheNovelCoronavirus2019nCoV2020; @verityEstimatesSeverityCoronavirus2020a] which cite an onset to admission of 2.7-5.9 days[@lintonIncubationPeriodOther2020;@tindaleEvidenceTransmissionCOVID192020;@sancheHighContagiousnessRapida] and onset to death delay of 16.1-17.8 days[@lintonIncubationPeriodOther2020; @sancheHighContagiousnessRapida]. Given our estimate of the incubation period is log normally distributed, it is unsurprising that the combination of incubation period and delay from onset to observation is also best described by log normal distributions (`r ref$fig("chess-delay")`, panel B), and with these we can apply non parametric back propagation to infer a time series of putative infections from the delayed observations we have available. 

In `r ref$fig("deconv-compare")` we bring all the different parts of the analysis together and compare the two approaches of formal estimation of *R~t~* using the generation interval and back propagation of case, admission or death counts to putative infection, versus an pragmatic estimation using the truncated re-sampled serial interval, and direct use of observation numbers, combined with the simple-but-incorrect adjustment to the resulting *R~t~* time series, shifting the date backwards by the mean of the delay distribution[@gosticPracticalConsiderationsMeasuring2020a]. 

<!-- In our pragmatic approach we make the assumption that case counts, admissions and deaths can be used as a direct proxy measure for infection events. The Cori method uses a Bayesian framework to update a prior probabilistic estimate of *R~t~*, on any given day, with information gained from the time series of infections in the epidemic to date, and the infectivity profile, to produce a posterior estimate of *R~t~*. With the assumption that prior and posterior estimates of *R~t~* are gamma distributed, the Cori method derives a closed form solution for the posterior distribution, which can be rapidly calculated. If equivalent information is contained within a time series of observation such as cases, admissions, or deaths we propose that there is no fundamental reason the Cori method cannot be extended to these data sets. However, if the time delay between infection and observation is changing over time then we may expect to see compression of the early part of the time series, and relaxation in later parts, and a resulting estimate of *R~t~* that is further away from 1 than estimates that would have been obtained from the true infection incidence had it been available.  -->

<!-- There is an open question about what proxy measure for the infectivity profile is most appropriate if using delayed observations in this way. One appealing possibility is the use of case interval ($SI_case$), admission interval ($SI_admission$), or death intervals ($SI_death$), rather than serial interval ($SI_onset$). These distributions (which are estimated in `r ref$sfig("observation-interval")`) are more normally distributed than serial interval or generation interval and have a very definite negative component, which cannot be integrated into the Cori method as-is, and pragmatic truncation of the distribution at zero would introduce an untenable increase in the mean of the distribution. We have adopted the empirical re-sampled serial interval distribution (truncated at zero) for infectivity profile in absence of a clear alternative, as this is a quantity that can be directly observed from the data, and has known limitations and uncertainty, and we note the potential for bias in our estimates of *R~t~* that this introduces. -->
In comparing formal versus pragmatic methods, given the number of moving parts in this comparison it is encouraging to see the level of agreement between the *R~t~* estimates from the two methods (`r ref$fig("deconv-compare")` panel B) in the early phase of the epidemic, and also to see that there is some similar structure of the *R~t~* time series in the later parts. This is particularly the case for estimates based on cases and admissions, but less so for deaths where the de-convolution time series shows additional features that are not obvious from the data. We have no gold standard in comparing the *R~t~* estimates for England, so we are limited in what we can conclude, but we do observe that *R~t~* estimates based on our attempt at de-convolution have more variability than ones from un-adjusted observations, and de-convolved estimates have additional features not present in the un-adjusted estimates. The de-convolved estimates of infection are also noted to run to the end of the time series, which is somewhat surprising as estimates of infection rates at the end of the time series should depend on data which has not yet been observed. This is a feature of the back propagation algorithm which needs to be used with caution as the resulting estimates of *R~t~* based on de-convolution for the latter part of the time series appear inconsistent with each other.

# Limitations

We did not fully quantify uncertainty in our analysis and estimation of *R~t~*. The informal approach to estimating *R~t~* has uncertainty arising from the serial interval distribution, and stochastic noise in the value of the observation in question. The formal approach involves uncertainty in the initial estimation of the serial interval, uncertainty in the incubation period, resulting in uncertainty in the generation interval, the back propagation itself is a source of uncertainty and involves uncertain time delay distributions which are in turn based on uncertain incubation period, and finally the stochastic noise in the observation under consideration. Accurately tracking the uncertainty of all these components into a final estimate remains a challenge.

Our estimates are based on the best available information at the present stage of the epidemic.  However the serial interval is not a fixed quantity and may be affected by behavioral changes such as case isolation, or social distancing. The assumption it is constant is questionable although we have very little hard evidence about how it may vary over time. Similarly time distributions from infection to case identification, admission and death are expected to be highly variable over the course of an outbreak. This has implications for their use in de-convolution, as changing time distributions will have a significant effect on the shape of inferred infection incidence curves, and the complexity of the de-convolution.

There are implicit selection biases in all the data sources we use. A large proportion of SARS-CoV-2 cases are asymptomatic[@baiPresumedAsymptomaticCarrier2020; @huClinicalCharacteristics242020; @mizumotoEstimatingAsymptomaticProportion2020]. It is highly likely that these people participate in transmission chains, and they may do so with a very different infectivity profile to those that are symptomatic, however we have no information about these people in the data sets, and hence all the estimates presented here could be quite different, when asymptomatic cases are taken into consideration.

Our approach in estimating key parameters has been to combine different data sets, which come from different international sources, and which have potentially different biases. In combining data sets we assume that time delays are independent of one another, and can be combined randomly, as we have no other evidence to the contrary. This assumption is questionable, as physiologically we can imagine that patients with a long incubation period, for example, may well have a longer period from symptom onset to admission, for example. This could have unpredictable effects on our estimates of time delays but the most likely is that our estimated variance is too small as a result.

<!-- In assessing the impact of differences in infectivity profile choices and different sources of observed incidence or inferred infections, we use a real life example of the outbreak in the UK. Whilst this is qualitatively interesting, and grounds the discussion in reality, as there is no gold standard to compare to, we are limited in our ability to conclude anything about the relative benefits of different approaches. Also as the outbreak was relatively stable in the UK over the period we considered, there are limited significant features to compare. An opportunity for further work exists in analysing a simulation with known epidemiological parameters that vary over time and assessing the performance of different methods. -->

# Conclusions

We argue that our estimates for the statistical distributions of these key parameters or serial interval, incubation period, generation interval, for the UK, along with their uncertainty represent the best available estimates for the UK, given the current state of knowledge.

As there is a wide range of candidate values for these quantities, we have assessed the bias that the variation in choice of parameters introduces to *R~t~* estimation, when using the forward equation method. Whilst these introduce significant variation in the worst case scenario, we find that even large differences in infectivity profile can have only small impacts on estimates of *R~t~* when *R~t~* is close to 1. Larger values for the mean of the infectivity profile appear to result in *R~t~* estimates that are further away from 1. This is a relatively reassuring finding in that the answer to the key question "is the epidemic under control?" is insensitive to the mean of the infectivity profile.

Using more formal methods for estimating *R~t~* by back propagation inference of infection rate and estimates of the generation interval, produces *R~t~* estimates that are more variable when compared to the more pragmatic direct use of case counts as a proxy for infection. Both methods agree on when the epidemic crossed the *R~t~* threshold of 1. However there is considerable uncertainty in the quantities needed to perform the back propagation, and we are not able to ensure that all this uncertainty could be faithfully quantified in our resulting *R~t~* estimates. We did not set out to assess whether one method is better than another, and this would be a natural extension to this work, however we note that new methods for combining back propagation with estimation of *R~t~* are under active development[@abbottEpiNow2EstimateRealtime2020; @abbottEstimatingTimevaryingReproduction2020] and may very well address such further questions.

# References

<div id="refs"></div>

\newpage
# Impact of uncertainty in serial interval, generation interval, incubation period and delayed observations in estimating the reproduction number for COVID 19

Robert Challen^1,2^; Ellen Brooks-Pollock^3^; Krasimira Tsaneva-Atanasova^1,4,5^; Leon Danon^3,4,5^

1) EPSRC Centre for Predictive Modelling in Healthcare, University of Exeter, Exeter, Devon, UK.
2) Somerset NHS Foundation Trust, Taunton, Somerset, UK.
3) Bristol Medical School, Population Health Sciences, University of Bristol, Bristol, UK.
4) The Alan Turing Institute, British Library, 96 Euston Rd, London NW1 2DB, UK.
5) Data Science Institute, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, UK. 

# Supplementary material

## `r cap$stab("dfit-serial-interval-ff100","Parametererised serial interval distributions from FF100. Gamma and Negative Binomial estimates are from data truncated at zero. AIC estimates are not comparable to those for Normal distribution which is fitting all data, including negative serial intervals, and hence has a lower mean.")`

```{r}
si3$dfit$printDistributionDetail() %>% ungroup() %>% select(N=n, AIC=aic, Distribution, `Parameter / Moment` = param, `Mean Â± SD (95% CI)`) %>% group_by(Distribution, AIC, N) %>% arrange(AIC) %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/TableS1_ParametersFF100SerialInterval")
```

## `r cap$sfig("meta-analysis","Forest plot for serial interval studies for A, the mean and B, the standard deviation of the serial interval, using the normal mixture random effect model, and from studies identified in the literature which assume a Gamma distributed serial interval")`

```{r}
library(metaplus)
tmp = serialIntervals %>% mutate(yi = mean_si_estimate, sei = (mean_si_estimate_high_ci-mean_si_estimate_low_ci)/3.92) %>% filter(!is.na(sei)) %>% filter(assumed_distribution == "gamma" & estimate_type %>% stringr::str_starts("serial"))
  meanfit = suppressWarnings(metaplus::metaplus(tmp$yi, tmp$sei, slab=tmp$label, random="mixture"))
      
plot(meanfit, cex=0.6, main="mean")

```

```{r}
tmp2 = serialIntervals %>% mutate(yi = std_si_estimate, sei = (std_si_estimate_high_ci-std_si_estimate_low_ci)/3.92) %>% filter(!is.na(sei)) %>% filter(assumed_distribution == "gamma" & estimate_type %>% stringr::str_starts("serial"))
sdfit = suppressWarnings(metaplus::metaplus(tmp2$yi, tmp2$sei, slab=tmp2$label, random="mixture"))
  
plot(sdfit, cex=0.6, main="std dev")

```


## `r cap$stab("dfit-serial-interval-resample","Parametererised serial interval distributions from resampling the literature. Gamma and Negative Binomial estimates are from data truncated at zero. AIC estimates are not comparable to those for Normal distribution which is fitting all data, including negative serial intervals, and hence has a lower mean.")`

```{r}
si1$dfit$printDistributionDetail() %>% ungroup() %>% select(N=n, AIC=aic, Distribution, `Parameter / Moment` = param, `Mean Â± SD (95% CI)`) %>% group_by(Distribution, AIC, N) %>% arrange(AIC) %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/TableS2_ParametersResampledSerialInterval")
```

## `r cap$stab("incub-period-detail","Distribution details for estimated incubation period distributions reconstructed from Open COVID-19 Data Working Group and from FF100 data")`

```{r}
bind_rows(
  incubFF100Fit$printDistributionDetail() %>% mutate(source = "FF100"),
  bopFit$printDistributionDetail() %>% mutate(source = "Open COVID-19 Data Working Group")
) %>% ungroup() %>% select(Source = source, N = n, AIC = aic, Distribution, `Parameter / Moment` = param, `Mean Â± SD (95% CI)`) %>% arrange(N, AIC, Distribution) %>%
  group_by(N, Source, AIC, Distribution) %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/TableS3_IncubationPeriodsBeOutbreakPreparedAndFF100_Detail")
```

## `r cap$stab("chess-delay-params","Time delay distributions estimated from CHESS data set, for both transitions from disease onset to case, admission or death, and presumed infection and case, admission or death")`

```{r}
suppTable = bind_rows(
  onsetFit$printDistributionDetail(),
  infectionToObservationFit$printDistributionDetail()
) %>% ungroup() %>% select(-dist,-bic,-loglik,-mean,-sd,-lower,-upper,-shift) %>% rename(Transition = transition, Parameter= param, AIC= aic, N=n)  %>% group_by(Transition, N, AIC, Distribution) %>% arrange(AIC)
suppTable %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/TableS4_ChessTimeDelayDistributionsDetail",defaultFontSize = 7)
```


## `r cap$sfig("observation-interval","Time intervals between infector-infectee observations")`

These distributions are based on the joint probability of serial interval (between onset) estimated from our literature re-sampling analysis and delay distributions from onset to observation, estimated from the CHESS data set. The joint probability is estimated as the combination of bootstrapped samples of raw data assuming these are independent (see discussion section).

$$
\begin{aligned}
SI_{observation,X \to Y} = SI_{onset, X \to Y} + T_{onset \to observation,Y} - T_{onset \to observation,X}
\end{aligned}
$$
In which the quantity $T_{onset \to observation,Y} - T_{onset \to observation,X}$ is essentially an error term with zero mean. We note that the un-truncated normal distributions mean is approximately equal for all 4 distributions, around 4.5 to 4.8, as the skew of the distribution affects the fitting process. In this figure the distributions were fitted using a maximum goodness of fit estimator. The serial interval for onset estimated in the main paper is included for comparison.


```{r}

simulatedSI = rawDelay %>%
  mutate(between = transition %>% stringr::str_remove("infection to")) %>% #actually this is not infection to anything, its observation to observation
  group_by(between,bootstrapNumber) %>%
  group_modify(function(d,g,...) {
    # each d here is a single bootstrap of delay from and to
    # grab a resampled serial interval bootstrap
    siEsts = si1$bootstrapSamples %>% filter(bootstrapNumber == g$bootstrapNumber) %>% pull(value) %>% sample(nrow(d))
    d = d %>% mutate(
      siOriginal = siEsts,
      siTo = siEsts+delayOffset
    )
    return(d)
  }) %>%
  select(between, bootstrapNumber,siOriginal,siTo,delayOffset) %>%
  bind_rows(
    si1$dfit$groupedDf %>% mutate(
      between = "onset",
      siOriginal = value,
      siTo = value,
      delayOffset = 0
    ) %>% select(-value)
  )

simulatedSIFit  = DistributionFit$new(c("norm","gamma","lnorm"))

simulatedSIFit$fromBootstrappedData(simulatedSI %>% group_by(between) %>% select(bootstrapNumber,siTo), valueExpr = siTo, method="mge") #method="qme", probs=seq(0.4,0.6,by = 0.02))
fig7 = simulatedSIFit$plot(xlim=c(-7,28),summary=TRUE)+xlab("Time interval")
fig7 %>% saveHalfPageFigure("~/Dropbox/covid19/serial-interval/FigS1_ObservationIntervalDistributions")
```



## `r cap$sfig("parameter-distributions","The distribution of the parameters of the fitted generation interval estimates")`

```{r}
# export distribution parameters
parameterDistributions = si4$dfit$bootstraps %>% pivot_wider(names_from = "param",values_from = "value") %>% 
  mutate(mean = shape/rate, sd = sqrt(shape/(rate^2)))



p1 = ggplot(parameterDistributions,aes(x=mean,y=sd,colour=shape*rate))+geom_point(show.legend = FALSE)+scale_color_distiller(palette = "YlGnBu",trans="sqrt")
p1a = ggExtra::ggMarginal(p1,type="density",fill="grey90")

p2 = ggplot(parameterDistributions,aes(x=shape,y=rate,colour=shape*rate))+geom_point(show.legend = FALSE)+scale_color_distiller(palette="YlGnBu",trans="sqrt")
p2a = ggExtra::ggMarginal(p2,type="density",fill="grey90")

p3 = patchwork::wrap_elements(p1a)+patchwork::wrap_elements(p2a)+patchwork::plot_layout(ncol=2)
p3 %>% saveHalfPageFigure("~/Dropbox/covid19/serial-interval/FigS3_GenerationIntervalParameterDistributions")

```

Generation interval distribution parameterisation is useful for Bayesian approaches. We investigated the properties of the distributions and found the mean and standard deviation or shape and rate parameters of our estimates of the generation interval are not uniformly distributed through parameter space. 


<!-- # OLD -->


<!-- ## Serial interval standard deviation assuming delay to presentation -->

<!-- * Serial intervals are a convolution of generation intervals, with a delay depending on time to presentation -->
<!-- * In theory  -->
<!-- * Linton et al estimates time to presentation / time to death etc -->
<!-- * What effect does the serial interval of death-death or test-test events considered -->
<!-- * simulate delay process using a gamma distribution -->
<!-- * assume a known generation interval and simulate effects -->

<!-- ```{r, fig.cap="Parameter distributions of resampled serial invervals"} -->
<!-- estimates2 = DistributionFit$unconvertParameters(si1$dfit$bootstraps) %>% rename(value = mean) %>% bind_rows(si1$dfit$bootstraps) %>% filter(dist == "gamma") -->

<!-- quants = estimates2 %>% group_by(param) %>% summarise( -->
<!--   tibble( -->
<!--     q=c(0.025,0.1,0.5,0.9,0.975), -->
<!--     value=quantile(value,c(0.025,0.1,0.5,0.9,0.975)) -->
<!--   )) -->

<!-- p1 = ggplot(estimates2,aes(x=value,colour=param))+geom_density(show.legend = FALSE)+geom_vline(data=quants,mapping=aes(xintercept = value))+geom_text(data=quants,mapping=aes(x = value,label=q),y=Inf,hjust=1.1,vjust=1.1,angle=90, inherit.aes = FALSE)+facet_wrap(vars(param), scales = "free") -->

<!-- p1 %>% standardPrintOutput::saveThirdPageFigure("~/Dropbox/covid19/serial-interval/FigS1_ParameterDistrbituions") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- #tmp2 = si3$dfit$groupedDf %>% mutate( -->
<!-- #  SL = as.integer(date_onset.infectee - date_onset.infector)) -->

<!-- #panel1 = ggplot(tmp2) + geom_bar(width=0.7,aes(x=SL)) + xlab("days") -->

<!-- # tmp2 = ff100 %>% mutate( -->
<!-- #   EL = 0L,  -->
<!-- #   ER = as.integer(date_exposure_last - date_exposure_first), -->
<!-- #   SL = as.integer(date_onset - date_exposure_first), -->
<!-- #   SR = as.integer(date_onset - date_exposure_first+1), -->
<!-- #   type=0L) %>% mutate(ER = ifelse(ER>SL,SL,ER)) %>% select(EL,ER,SL,SR,type) %>% filter(SL>0) -->
<!-- # tmp3 = as.matrix(tmp2[,c("EL","ER","SL","SR","type")]) -->

<!-- # tmp2 = tmp2 %>% filter(SL>0) -->
<!-- # tmp3 = as.matrix(tmp2[,c("EL","ER","SL","SR","type")]) -->
<!-- #  -->
<!-- # MCMC_seed <- 1 -->
<!-- # overall_seed <- 2 -->
<!-- # mcmc_control <- EpiEstim::make_mcmc_control(seed = MCMC_seed,  -->
<!-- #                                   burnin = 1000) -->
<!-- # dist <- "G" # fitting a Gamma dsitribution for the SI -->
<!-- # config <- EpiEstim::make_config(list(si_parametric_distr = dist, -->
<!-- #                            mcmc_control = mcmc_control, -->
<!-- #                            seed = overall_seed,  -->
<!-- #                            n1 = 50,  -->
<!-- #                            n2 = 50)) -->
<!-- #  -->
<!-- # # rm(`%in%`) = function(x,y) { -->
<!-- # #   return(sapply(x, function(x1) {any(y == x1)})) -->
<!-- # # } -->
<!-- #  -->
<!-- # ## first estimate the SI distribution using function dic.fit.mcmc fron  -->
<!-- # ## coarseDataTools package: -->
<!-- # n_mcmc_samples <- config$n1*mcmc_control$thin -->
<!-- #  -->
<!-- # SI_fit = coarseDataTools::dic.fit.mcmc(dat = tmp3, -->
<!-- #                   dist = "G",#off1G", -->
<!-- #                   init.pars = EpiEstim::init_mcmc_params(tmp2, dist), -->
<!-- #                   burnin = mcmc_control$burnin, -->
<!-- #                   n.samples = n_mcmc_samples, -->
<!-- #                   seed = mcmc_control$seed) -->
<!-- #  -->
<!-- # si_sample <- EpiEstim::coarse2estim(SI_fit, thin = mcmc_control$thin)$si_sample -->
<!-- #  -->
<!-- # calcGammaMean = function(x) { -->
<!-- #   shape = SI_fit@ests["shape",x] -->
<!-- #   scale = SI_fit@ests["scale",x] -->
<!-- #   out = list() -->
<!-- #   out$mean = shape*scale -->
<!-- #   out$sd = sqrt(shape*scale^2) -->
<!-- #   return(out) -->
<!-- # } -->
<!-- #  -->
<!-- # UKSIConfig = EpiEstim::make_config( -->
<!-- #   mean_si = calcGammaMean("est")[["mean"]], -->
<!-- #   std_si = calcGammaMean("est")[["sd"]], -->
<!-- #   min_mean_si = calcGammaMean("CIlow")[["mean"]], -->
<!-- #   min_std_si = calcGammaMean("CIlow")[["sd"]], -->
<!-- #   max_mean_si = calcGammaMean("CIhigh")[["mean"]], -->
<!-- #   max_std_si = calcGammaMean("CIhigh")[["sd"]], -->
<!-- #   #TODO: the following are not going to be used unless we apply this -->
<!-- #   std_mean_si = (calcGammaMean("CIhigh")[["mean"]]-calcGammaMean("CIlow")[["mean"]])/3.96,  -->
<!-- #   std_std_si = (calcGammaMean("CIhigh")[["sd"]]-calcGammaMean("CIlow")[["sd"]])/3.96, -->
<!-- #   method = "uncertain_si" -->
<!-- # ) -->
<!-- #  -->
<!-- # gammaMean = sprintf("%1.2f (%1.2f-%1.2f)", -->
<!-- #   calcGammaMean("est")[["mean"]], -->
<!-- #   calcGammaMean("CIlow")[["mean"]], -->
<!-- #   calcGammaMean("CIhigh")[["mean"]] -->
<!-- # ) -->
<!-- #  -->
<!-- # gammaShape = sprintf("%1.2f (%1.2f-%1.2f)", -->
<!-- #   SI_fit@ests["shape","est"], -->
<!-- #   SI_fit@ests["shape","CIlow"], -->
<!-- #   SI_fit@ests["shape","CIhigh"] -->
<!-- # ) -->
<!-- #  -->
<!-- # gammaScale = sprintf("%1.2f (%1.2f-%1.2f)", -->
<!-- #   SI_fit@ests["scale","est"], -->
<!-- #   SI_fit@ests["scale","CIlow"], -->
<!-- #   SI_fit@ests["scale","CIhigh"] -->
<!-- # ) -->
<!-- #  -->
<!-- # gammaSd = sprintf("%1.2f (%1.2f-%1.2f)", -->
<!-- #   calcGammaMean("est")[["sd"]], -->
<!-- #   calcGammaMean("CIlow")[["sd"]], -->
<!-- #   calcGammaMean("CIhigh")[["sd"]] -->
<!-- # ) -->
<!-- #  -->
<!-- # panel2 = (ggplot(tmp2, aes(x=SL)) + geom_histogram(aes(y=..density..),fill=NA,colour = "black", binwidth=1)+ #,width=0.7) + -->
<!-- #     geom_line(data = tibble( -->
<!-- #       x=seq(0,10,length.out = 101), -->
<!-- #       y=dgamma(seq(0,10,length.out = 101), shape = SI_fit@ests["shape","est"], scale = SI_fit@ests["scale","est"]) -->
<!-- #     ), aes(x=x,y=y), inherit.aes = FALSE, colour="blue")+ -->
<!-- #     geom_line(data = tibble( -->
<!-- #       x=seq(0,10,length.out = 101), -->
<!-- #       y=dgamma(seq(0,10,length.out = 101), shape = SI_fit@ests["shape","CIlow"], scale = SI_fit@ests["scale","CIlow"]) -->
<!-- #     ), aes(x=x,y=y), inherit.aes = FALSE, colour="blue",linetype="dashed")+ -->
<!-- #     geom_line(data = tibble( -->
<!-- #       x=seq(0,10,length.out = 101), -->
<!-- #       y=dgamma(seq(0,10,length.out = 101), shape = SI_fit@ests["shape","CIhigh"], scale = SI_fit@ests["scale","CIhigh"]) -->
<!-- #     ), aes(x=x,y=y), inherit.aes = FALSE, colour="blue",linetype="dashed")+ -->
<!-- #     annotate("text", x = 10, y = 0.5, label = paste0("Mean: ",gammaMean,"\nSD: ",gammaSd,"\nShape: ",gammaShape,"\nScale: ",gammaScale),hjust="inward",vjust="inward")+ -->
<!-- #     xlab("days") -->
<!-- # )  -->
<!-- ``` -->

<!-- This used fitted distributions to do combination and has been replacedf by code using raw estimates. -->

<!-- ```{r} -->

<!-- # #glimpse(symptomaticToCaseModels %>% filter(aic == min(aic))) -->
<!-- # # select best fit (by AIC) for days symptomatic to case. -->
<!-- # # get 200 random samples for each bootstrap -->
<!-- # # split into 2 groups and exclude any values with number of days > 30 -->
<!-- # # calculate the difference -->
<!-- #  -->
<!-- # ### From infgection  -->
<!-- # # onsetToTestFit$filterModels(aic == min(aic)) -->
<!-- # # onsetToTestFit$generateSamples(sampleExpr = 2000) -->
<!-- # fromInfectionDelay = function(distFit) { -->
<!-- #   distFit2 = distFit$clone() -->
<!-- #   distFit2$filterModels(aic == min(aic)) -->
<!-- #   distFit2$generateSamples(sampleExpr = 2000) -->
<!-- #  -->
<!-- #    -->
<!-- #   # get samples from distibution and split into 2 groups - one for index patient and one for affected patient -->
<!-- #   distSamples = distFit2$samples %>% -->
<!-- #     mutate(sampleCat = (sampleNumber-1) %/% 1000 + 1, sampleNumber = sampleNumber %% 1000) %>% -->
<!-- #     pivot_wider(names_from = sampleCat, values_from = value, names_prefix = "delay") %>% -->
<!-- #     mutate(to = stringr::str_replace(transition,"onset to ","")) -->
<!-- #    -->
<!-- #   # delay from infection -->
<!-- #   # the join adds in incubation  -->
<!-- #   delayDist = incubDist %>% select(-transition, -dist, -to) %>%  -->
<!-- #     inner_join(distSamples, by=c("sampleNumber","bootstrapNumber")) %>%  -->
<!-- #     mutate(from="infection") -->
<!-- #   delayDist = delayDist %>% mutate(delayOffset = delay2+incub2-delay1) -->
<!-- #    -->
<!-- #   # delay from onset -->
<!-- #   delayOnsetDist = distSamples %>% mutate(delayOffset = delay2-delay1, from="onset") -->
<!-- #    -->
<!-- #   return(delayDist %>% bind_rows(delayOnsetDist)) -->
<!-- # } -->
<!-- #  -->
<!-- # relativeDelays = bind_rows( -->
<!-- #   incubDist, -->
<!-- #   onsetToTestFit %>% fromInfectionDelay(), -->
<!-- #   onsetToAdmissionFit %>% fromInfectionDelay(), -->
<!-- #   #onsetToTestResultFit %>% fromInfectionDelay(), -->
<!-- #   onsetToDeathFit %>% fromInfectionDelay() -->
<!-- # )  -->
<!-- #  -->
<!-- # # relative delays df here has distributions for a whole number of bootstraps. -->
<!-- # # we could use this to fit models. and determine uncertainty on densitites. -->
<!-- # # aggregating over all bootstraps and plotting all results as density.  -->
<!-- #  -->
<!-- # #ggplot(relativeDelays %>% filter(delayOffset > -25 & delayOffset < 25), aes(x=delayOffset, colour = transition, fill=transition))+geom_density(alpha = 0.1,show.legend = FALSE)+facet_wrap(vars(transition)) -->
<!-- # ggplot(relativeDelays %>% filter(delayOffset > -45 & delayOffset < 45), aes(x=delayOffset, colour = to))+geom_density()+facet_wrap(vars(from)) -->

<!-- ``` -->


<!-- Hospital admission generally predates case detection in this data set as hospital based. No clear reason to treat case identification and hospital admission as different time points. -->


<!-- ```{r} -->
<!-- symptomaticToAdmission = CHESSClean %>% filter(age>10 & !is.na(estimateddateonset) & !is.na(hospitaladmissiondate)) %>%  -->
<!--   srv$generateNoAgeSurvivalData( -->
<!--     idVar = caseid, -->
<!--     startDateVar = estimateddateonset,  -->
<!--     endDateExpr = hospitaladmissiondate, -->
<!--     statusExpr = 1, -->
<!--     statusLabels = "admitted", -->
<!--     censoredDateExpr = NA -->
<!--   ) #%>% filter(time < 100 & time > 0) -->

<!-- symptomaticToAdmission = symptomaticToAdmission %>% filter(time < 100 & time > 0) -->
<!-- symptomaticToAdmissionModels = symptomaticToAdmission %>% srv$fitModels(models, shifted=1) -->
<!-- symptomaticToAdmissionModels %>% srv$plotModels(symptomaticToAdmission) %>% standardPrintOutput::saveSixthPageFigure("~/Dropbox/covid19/serial-interval/FigS1_symptomaticToAdmission") -->
<!-- symptomaticToAdmissionModels %>% mutate(valueCI = sprintf("%1.3f \U00B1 %1.3f (%1.3f; %1.3f)", mean, sd, lower, upper), loglik = max(loglik)) %>% ungroup() %>% select( -->
<!--   `Distribution` = dist, -->
<!--   `Log-likelihood` = loglik, -->
<!--   `AIC` = aic, -->
<!--   `Parameter` = param, -->
<!--   `Value (95% CI)` = valueCI, -->
<!-- ) %>% group_by(`Distribution`,`Log-likelihood`,`AIC`) %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/TableS1_symptomaticToAdmissionParams") -->

<!-- ``` -->

<!-- * Estimate gamma from MCMC -->

<!-- ```{r} -->
<!-- # minimal generation interval deconvolution -->
<!-- # create samples from incubation period offset -->


<!-- #sampleSize = si1$dfit$groupedDf %>% group_by(bootstrapNumber) %>% count() %>% pull(n) %>% max() -->
<!-- sampleSize = nrow(si1$dfit$fitData) -->

<!-- incub2Fit = lauerFit$clone() -->
<!-- incub2Fit$bootstraps = incub2Fit$bootstraps %>% inner_join( -->
<!--   si1$dfit$groupedDf %>% group_by(bootstrapNumber) %>% count(), -->
<!--   by = "bootstrapNumber" -->
<!--   ) -->
<!-- incub2Fit$generateSamples(sampleExpr = n * 2) -->
<!-- incub2Samples = incub2Fit$samples %>% filter(dist == "lnorm") %>% -->
<!--     mutate(sampleCat = sampleNumber %% 2 + 1, sampleNumber = (sampleNumber+1) %/% 2) %>% -->
<!--     pivot_wider(names_from = sampleCat, values_from = value, names_prefix = "delay") %>% -->
<!--     mutate(delay = delay2-delay1) -->

<!-- # get the  -->
<!-- ordered = incub2Samples %>% group_by(bootstrapNumber) %>% arrange(delay) %>% mutate(order=row_number()) -->


<!-- tmp = si1$dfit$groupedDf %>% group_by(bootstrapNumber) %>% arrange(value) %>% mutate(order = row_number()) %>% inner_join(ordered, by=c("bootstrapNumber","order")) -->

<!-- ggplot(tmp, aes(x=value-delay))+geom_density() -->
<!-- #TODO: -->
<!-- # a bootstrapping resampling which respect the fact that si1 value minus incubFit delay must be greater than zero. -->

<!-- ``` -->
<!-- ```{r} -->
<!-- #reticulate::install_miniconda() -->
<!-- #reticulate::conda_create("r-tensorflow", packages=c("python=3.7","tensorflow=1.14","pyyaml","requests","Pillow","pip","numpy=1.16")) -->
<!-- reticulate::use_condaenv(condaenv = 'r-tensorflow', required = TRUE) -->
<!-- #reticulate::py_install("tensorflow-probability=0.7", "r-tensorflow") -->

<!-- # variables & priors -->
<!-- #mean <- greta::uniform(0,10) # -->

<!-- dof = 5 -->

<!-- sd <- greta::gamma(rate = 1/2, shape = dof/2) -->
<!-- sdOfMean   <- greta::cauchy(0, 3, truncation = c(0, Inf)) -->
<!-- mean = greta::normal(mean=si1$dfit$groupedDf$value, sd=10, truncation = c(0, Inf)) -->

<!-- genInterval <- greta::gamma(shape = mean^2/sd^2, rate=mean/sd^2) -->

<!-- pt1Delay <- greta::lognormal( -->
<!--   meanlog = lauerFit$fittedModels %>% filter(dist=="lnorm" & param=="meanlog") %>% pull(mean), -->
<!--   sdlog = lauerFit$fittedModels %>% filter(dist=="lnorm" & param=="sdlog") %>% pull(mean) -->
<!-- ) -->

<!-- pt2Delay <- greta::lognormal( -->
<!--   meanlog = lauerFit$fittedModels %>% filter(dist=="lnorm" & param=="meanlog") %>% pull(mean), -->
<!--   sdlog = lauerFit$fittedModels %>% filter(dist=="lnorm" & param=="sdlog") %>% pull(mean) -->
<!-- ) -->

<!-- # linear predictor -->
<!-- mu <- genInterval + pt1Delay - pt2Delay -->

<!-- # observation model -->
<!-- greta::distribution(si1$dfit$groupedDf$value) <- greta::normal(mu,sdOfMean) -->

<!-- #m <- greta::model(mean, sd, sdOfMean) -->
<!-- m <- greta::model(mean, sd, sdOfMean) -->
<!-- draws <- greta::mcmc(m, n_samples = 2000, chains = 4) -->
<!-- bayesplot::mcmc_trace(draws) -->
<!-- summary(draws) -->

<!-- ``` -->

<!-- 
#TODO: convert this to stan 
# DECONVOLUTION: https://stackoverflow.com/questions/12462919/deconvolution-with-r-decon-and-deamer-package
# https://www.rdocumentation.org/packages/surveillance/versions/1.12.1/topics/backprojNP
deamer / decon
-->







<!-- ```{r skip=TRUE} -->
<!-- #TODO: from Lauer's original data -->

<!-- # Reconstruct serial interval -->
<!-- # https://www.acpjournals.org/doi/10.7326/M20-0504#t4-M200504 -->
<!-- # Original data here: https://github.com/HopkinsIDD/ncov_incubation/tree/master/data -->
<!-- # Lauer incubation periods -->
<!-- lauerIncub = tibble::tribble( -->
<!--   ~dist, ~param, ~paramValue, -->
<!--   "lnorm","meanlog","1.621 (1.504â€“1.755)", -->
<!--   "lnorm","sdlog","0.418 (0.271â€“0.542)", -->
<!--   "gamma", "shape", "5.807 (3.585â€“13.865)",  -->
<!--   "gamma", "scale", "0.948 (0.368â€“1.696)", -->
<!--   "weibull", "shape", "2.453 (1.917â€“4.171)",  -->
<!--   "weibull", "scale", "6.258 (5.355â€“7.260)", -->
<!-- #  "erlang", "shape", "6 (3â€“11)", -->
<!-- #  "erlang", "scale", "0.880 (0.484â€“1.895)" -->
<!-- ) -->

<!-- lauerIncub = lauerIncub %>% mutate( -->
<!--   paramValueList = lapply(stringr::str_extract_all(paramValue, "[0-9]+\\.?[0-9]*"),as.numeric) -->
<!-- ) %>% mutate( -->
<!--   mean = map_dbl(paramValueList, ~.x[1]), -->
<!--   lower = map_dbl(paramValueList, ~.x[2]), -->
<!--   upper = map_dbl(paramValueList, ~.x[3]), -->
<!--   sd = (upper-lower)/3.96 # THIS ASSUMPTION IS WHAT CAUSES DIFFERENCE IN PUBLISHED AND BOOTSTRAPPED MEANS AND RESULTS IN RESAMPLING -->
<!-- ) -->

<!-- lauerFit = DistributionFit$new(distributions = unique(lauerIncub$dist)) -->

<!-- lauerIncub %>% group_by(dist) %>% group_map(function(d,g,...) { -->
<!--   lauerFit$withSingleDistribution(dist = g$dist,paramDf = d %>% select(param,mean,sd,lower,upper),bootstraps = 1000) -->
<!--   return(NULL) -->
<!-- }) #%>% invisible() -->

<!-- # From original data -->

<!-- lauerRaw = readr::read_csv("https://raw.githubusercontent.com/HopkinsIDD/ncov_incubation/master/data/nCoV-IDD-traveler-data.csv") -->
<!-- for (col in c("ER","EL","SL","SR","PR")) { -->
<!--   col = as.symbol(col) -->
<!--   lauerRaw = lauerRaw %>% mutate(!!col := as.numeric(as.Date(stringr::str_extract(!!col,"[0-9]{4}-[0-9]{2}-[0-9]{2}")))) -->
<!-- } -->
<!-- lauerRaw = lauerRaw %>% mutate(type = 0L, -->
<!--   ER=ER+1,   -->
<!--   SR=SR+1 -->
<!-- ) -->

<!-- assumedEarliest = as.numeric(as.Date("2019-12-01")) -->
<!-- #https://github.com/HopkinsIDD/ncov_incubation/blob/master/manuscript/nCoV_Incubation.Rmd -->
<!-- lauerRawProc <- lauerRaw %>%  -->
<!--              # if EL is missing or before 1 Dec 2019, use 1 Dec 2019 -->
<!--              mutate( -->
<!--                EL = ifelse(is.na(EL) | EL < assumedEarliest, assumedEarliest, EL), -->
<!--                # if SR is missing, use PR -->
<!--                SR = ifelse(is.na(SR), PR, SR), -->
<!--                # if ER is missing, use SR; if SL is missing, use EL -->
<!--                ER=if_else(is.na(ER) | ER>SR, SR, ER), -->
<!--                SL=if_else(is.na(SL) | SL<EL, EL, SL) -->
<!--              ) -->
<!-- lauerRawProc = lauerRawProc %>% mutate( -->
<!--     EL = EL-assumedEarliest, -->
<!--     ER = ER-assumedEarliest, -->
<!--     SL = SL-assumedEarliest, -->
<!--     SR = SR-assumedEarliest, -->
<!--     E_int=ER-EL, -->
<!--     S_int=SR-SL -->
<!--   ) %>% -->
<!--     # any entries missing EL, ER, SL, or SR -->
<!--   filter( -->
<!--     !is.na(EL) & !is.na(ER) & !is.na(SL) & !is.na(SR) -->
<!--   ) %>%  -->
<!--     # remove entries that haven't been reviewed by two people -->
<!--   filter(!is.na(REVIEWER2), REVIEWER2!="NA") %>%  -->
<!--     # remove entries with exposure/onset intervals less than 0 -->
<!--     # remove entries where ER greater than SR or EL greater than SL -->
<!--   filter(E_int > 0, S_int > 0, ER<=SR, SL>=EL) %>% -->
<!--   mutate(type=0L) %>% -->
<!--   select(ER,EL,SR,SL,type) -->

<!-- tmp3 = as.matrix(lauerRawProc[,c("EL","ER","SL","SR","type")]) -->

<!-- ## estimate the SI distribution using function dic.fit.mcmc fron  -->
<!-- ## coarseDataTools package: -->
<!-- # n_mcmc_samples <- config$n1*mcmc_control$thin -->
<!-- #  -->
<!-- SI_fit = coarseDataTools::dic.fit(dat = tmp3, -->
<!--                    dist = "G", -->
<!--                    n.boots = 50) -->

<!-- # fitdistrplus -->
<!-- lauerRaw = lauerRaw %>% mutate( -->
<!--   left = pmin(SL,SR,na.rm = TRUE)-(EL+ER)/2, -->
<!--   right = pmin(SL,SR,na.rm = TRUE)-(EL+ER)/2, -->
<!--   ) %>% mutate( -->
<!--     left = ifelse(left < 0 | is.na(left) , 0 ,left), -->
<!--     right = ifelse(right < 0 , 0 ,right) -->
<!--   ) %>% filter(!(is.na(left) & is.na(right))) -->

<!-- lauerRawFit = DistributionFit$new(c("lnorm","gamma","nbinom","weibull")) -->
<!-- lauerRawFit$fromCensoredData(lauerRaw, lowerValueExpr = left, upperValueExpr = right) -->
<!-- lauerRawFit$printDistributionSummary() -->
<!-- ``` -->



<!-- ```{r} -->
<!-- onsetToTestResult = CHESSClean %>%  -->
<!--     filter(age>10 & !is.na(estimateddateonset) & !is.na(labtestdate)) %>%  -->
<!--     mutate( -->
<!--       transition = "onset to test result", -->
<!--       time = as.integer(labtestdate - estimateddateonset) -->
<!--     ) %>% select(caseid,transition,time) %>% filter(time < 28 & time > -14) %>% group_by(transition) -->

<!-- onsetToTestResultFit = DistributionFit$new(distributions = c("lnorm","gamma","weibull","nbinom"))$fromUncensoredData(onsetToTestResult, valueExpr = time, truncate = TRUE, bootstraps = 100) -->
<!-- plots$onsetToTestResult = onsetToTestResultFit$plot(xlim = c(0,20))+xlab("time delay") -->
<!-- tables$onsetToTestResult = onsetToTestResultFit$printDistributionDetail() -->
<!-- ``` -->


<!-- * Test to admission -->
<!-- * Potentially complex as admission may occur before test comes back positive. -->
<!-- * Also issues with hospital acquired infections. -->
<!-- * Assume admission before 14 days is unrelated. cap at 100. -->

<!-- ```{r} -->
<!--   # Swab positive to admission -->
<!-- testToAdmission = CHESSClean %>%  -->
<!--     filter(age>10 & !is.na(infectionswabdate) & !is.na(hospitaladmissiondate)) %>%  -->
<!--     mutate( -->
<!--       transition = "test to admission", -->
<!--       time = as.integer(hospitaladmissiondate - infectionswabdate) -->
<!--     ) %>% select(caseid,transition,time) %>% filter(time < 100 & time > -14) -->
<!-- testToAdmissionFit = DistributionFit$new(distributions = c("norm","lnorm","gamma","weibull","exp"))$fromUncensoredData(testToAdmission, valueExpr = time, truncate = TRUE, bootstraps = 100) -->
<!-- testToAdmissionFit$plot(xlim = c(-14,25)) -->
<!-- testToAdmissionFit$printDistributionDetail() -->
<!-- ``` -->

<!-- ## Impact of delays on serial interval / Serial interval adjustments for non onset data -->

<!-- * Time delay of estimate of R_t depends on incubation period -->
<!-- * What is impact of these delay distributions on estimating R_t using the serial interval?  -->
<!-- * We look at: -->
<!-- * serial interval onset-onset + delay observation infectee - delay observation infector -->
<!-- * These can be though of as error functions, applied to serial interval distribution to produce serial interval distribution between deaths or admissions, or cases. -->
<!-- * We use our resampled SI estimates and chess delays -->

<!-- ```{r} -->

<!-- simulatedSI = rawDelay %>%  -->
<!--   filter(from=="onset") %>%  -->
<!--   group_by(from,to,bootstrapNumber) %>%  -->
<!--   group_modify(function(d,g,...) { -->
<!--     # each d here is a single bootstrap of delay from and to -->
<!--     # grab a resampled serial interval bootstrap -->
<!--     siEsts = si1$bootstrapSamples %>% filter(bootstrapNumber == g$bootstrapNumber) %>% pull(value) %>% sample(nrow(d)) -->
<!--     d = d %>% mutate( -->
<!--       siOriginal = siEsts, -->
<!--       siTo = siEsts+delayOffset -->
<!--     ) -->
<!--     return(d) -->
<!--   }) %>%   -->
<!--   select(between = to,bootstrapNumber,siOriginal,siTo,delayOffset) %>% -->
<!--   bind_rows( -->
<!--     si1$dfit$groupedDf %>% mutate( -->
<!--       between = "onset", -->
<!--       siOriginal = value, -->
<!--       siTo = value, -->
<!--       delayOffset = 0 -->
<!--     ) %>% select(-value) -->
<!--   ) -->

<!-- ``` -->


<!-- ```{r fig7} -->

<!-- simulatedSIFit  = DistributionFit$new(c("norm","gamma","nbinom")) -->

<!-- simulatedSIFit$fromBootstrappedData(simulatedSI %>% group_by(between) %>% select(bootstrapNumber,siTo), valueExpr = siTo, method="mge") #method="qme", probs=seq(0.4,0.6,by = 0.02)) -->
<!-- fig7 = simulatedSIFit$plot(xlim=c(-7,28))+xlab("Time interval") -->
<!-- fig7 %>% saveHalfPageFigure("~/Dropbox/covid19/serial-interval/Fig7_TimeIntervalDistributionsBetweenObservation") -->
<!-- ``` -->

<!-- `r cap$fig("observation-interval","Time intervals between infector-infectee observations")` -->

<!-- * Negative values for time intervals create poor gamma fit. -->
<!-- * Source of bias when considering estimating R_t using observations other than onset (and since onset is typically not reported this means all). -->
<!-- * EpiEstim assumes serial interval is >0 -->
<!-- * Gostic et al. suggest approach in deconvolution / generation interval. No good estimates of generation interval exist - can be derived by deconvolution of serial interval -->
<!-- * Assumption that serial interval >0 can be addressed in EpiEstim -->
<!-- * Quantification of bias -->
<!-- * ALternative approach is to use time interval between observations in lieu of SI  -->
<!-- * Use truncated normal distributions to model -->
<!-- * Understand that fraction of contributors to observed eventare not included in estimate -->

<!-- `r cap$tab("trunc-norm","Truncated normal parametrisation of serial observation interval distributions for different observations")` -->

<!-- ```{r table3} -->

<!-- # thinking here about -->
<!-- # tmp = simulatedSI %>% group_by(between,bootstrapNumber) %>% summarise( -->
<!-- #   negativeFrac = sum(ifelse(siTo < 1,1,0))/n(), -->
<!-- #   positiveFrac = sum(ifelse(siTo >= 1,1,0))/n(), -->
<!-- #   mean1 = mean(siTo), -->
<!-- #   sd1 = sd(siTo) -->
<!-- # ) %>% group_by(between) %>% summarise( -->
<!-- #   tibble( -->
<!-- #     param = c("positive fraction","negative fraction","mean","sd"), -->
<!-- #     mean = c(mean(positiveFrac),mean(negativeFrac),mean(mean1),mean(sd1)), -->
<!-- #     sd = c(sd(positiveFrac),sd(negativeFrac),sd(mean1),sd(sd1)), -->
<!-- #     lower = c(quantile(positiveFrac,0.025),quantile(negativeFrac,0.025),quantile(mean1,0.025),quantile(sd1,0.025)), -->
<!-- #     upper = c(quantile(positiveFrac,0.975),quantile(negativeFrac,0.975),quantile(mean1,0.975),quantile(sd1,0.975)) -->
<!-- #   ) -->
<!-- # ) -->

<!-- truncNorm = simulatedSI %>% group_by(between,bootstrapNumber) %>% summarise( -->
<!--   mean1 = mean(siTo), -->
<!--   sd1 = sd(siTo) -->
<!-- ) %>% group_by(between) %>% summarise( -->
<!--   meanOfMean = mean(mean1), -->
<!--   sdOfMean =sd(mean1), -->
<!--   lowerOfMean = quantile(mean1,0.025), -->
<!--   upperOfMean = quantile(mean1,0.975), -->
<!--   meanOfSd = mean(sd1), -->
<!--   sdOfSd =sd(sd1), -->
<!--   lowerOfSd = quantile(sd1,0.025), -->
<!--   upperOfSd = quantile(sd1,0.975), -->
<!-- ) %>% mutate( -->
<!--   `Mode (95% CI)` = sprintf("%1.2f (%1.2f; %1.2f)",meanOfMean,lowerOfMean,upperOfMean), -->
<!--   `SD (95% CI)` = sprintf("%1.2f (%1.2f; %1.2f)",meanOfSd,lowerOfSd,upperOfSd), -->
<!-- )  -->
<!-- truncNorm %>% write.csv(file=paste0(dpc$directory,"/","OBSERVATION_INTERVAL_TRUNC_NORM.csv")) -->
<!-- ukCovidObservationIntervals = truncNorm -->
<!-- usethis::use_data(ukCovidObservationIntervals, overwrite = TRUE) -->

<!-- truncNorm %>% arrange(meanOfSd) %>% select(Observation=between,`Mode (95% CI)`,`SD (95% CI)`) %>%  -->
<!--   standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/Table3_TruncatedNormal") -->


<!-- si4 = SerialIntervalProvider$truncatedNormals(dpc) -->

<!-- ``` -->

<!-- * Negative values of time interval between observations not accounted for in EpiEstim. -->
<!-- * We should anticipate this biases results using this estimation method as fraction of cases contributing to current obseved numbers of e.g. deaths have not themselves been observed.  -->
<!-- * Underestimates number of infectors, therefore overestimates *R~t~*. Size of overestimate depends on fraction of negative serial interval -->
<!-- * Also depends on dynamics of infection - will tend to overestimate *R~t~* in situation where infection numbers rising, and get more accurate when infection numbers falling rapidly. -->
<!-- * Shorter timescales show less effect. -->

<!-- `r cap$tab("correction","Proportion of serial observation interval that is negative")` -->

<!-- ```{r table4} -->

<!-- corrFac = simulatedSIFit$calculateCumulativeDistributions(q = 0) %>%  -->
<!--   filter(dist == "norm") %>% arrange(Mean.cumulative) %>% -->
<!--   mutate( -->
<!--     `Proportion negative SI (95% CI)` = sprintf("%1.2f (%1.2f; %1.2f)",Mean.cumulative, Quantile.0.025.cumulative, Quantile.0.975.cumulative), -->
<!--     `Naive correction factor` = sprintf("%1.2f",1-Mean.cumulative), -->
<!--   ) %>% -->
<!--   ungroup() %>% -->
<!--   select(`Time intervals between` = between, `Proportion negative SI (95% CI)`,`Naive correction factor`) -->
<!-- corrFac %>% standardPrintOutput::saveTable("~/Dropbox/covid19/serial-interval/Table4_CorrectionFactor") -->
<!-- ukCovidCorrectionFactor = corrFac -->
<!-- usethis::use_data(ukCovidCorrectionFactor, overwrite = TRUE) -->
<!-- ``` -->



<!-- ```{r} -->

<!-- ff100ts = ukts %>% tsp$estimateRt(quick=FALSE, serialIntervalProvider=si3) %>% tsp$adjustRtDates(window = 0) -->
<!-- resampledTs = ukts %>% tsp$estimateRt(quick=FALSE, serialIntervalProvider=si1) %>% tsp$adjustRtDates(window = 0,) -->
<!-- midmarketTs = ukts %>% tsp$estimateRt(quick=FALSE, serialIntervalProvider=si2) %>% tsp$adjustRtDates(window = 0) -->
<!-- truncNormTs = ukts %>% tsp$estimateRtWithAssumptions(quick=FALSE, serialIntervalProvider=si4) #%>% tsp$adjustRtDates() #%>% tsp$adjustRtCorrFac() #%>% tsp$adjustRtCorrFac()  -->

<!-- tmp = bind_rows( -->
<!--   ff100ts %>% mutate(source = "ff100"), -->
<!--   resampledTs %>% mutate(source = "resampled"), -->
<!--   midmarketTs %>% mutate(source = "midmarket"), -->
<!--   #truncNormTs %>% mutate(source = "truncated normals") -->
<!-- ) -->



<!-- tmp %>% filter(statistic == "case") %>% tsp$plotRt(colour = source,events = events)+scale_color_brewer(palette = "Set1",guide="none")+facet_wrap(vars(source),ncol = 1) -->


<!-- ``` -->

<!-- * Use empirical resampled distribution of the serial interval and Lauer's estimates of the incubation period to estimate a distribution for the generation interval --> -->
<!-- * THis is a deconvolution where difference of incubation period is error function -->

<!-- ```{r} -->
<!-- generationInterval = si1$bootstrapSamples %>% group_by(bootstrapNumber) %>% group_modify(function(d,g,...) { -->
<!--   message(".",appendLF = FALSE) -->
<!--   errors = incubDist %>% filter(from=="infection" & to =="onset" & bootstrapNumber==g$bootstrapNumber) %>% pull(delayOffset) -->
<!--   tmp = deamer::deamerSE( -->
<!--     y = d$value+runif(length(d$value),-0.5,0.5),  -->
<!--     errors = errors, from = 0, to = 21, grid.length = 22) -->
<!--   return(tibble( -->
<!--     density = as.vector(tmp$f), -->
<!--     x = tmp$supp -->
<!--   )) -->
<!-- }) -->


<!-- pltGenInt = generationInterval %>% group_by(x) %>% summarise( -->
<!--   y = mean(density), -->
<!--   ymin1 = quantile(density,0.025), -->
<!--   ymax1 = quantile(density,0.975), -->
<!--   ymin2 = quantile(density,0.25), -->
<!--   ymax2 = quantile(density,0.75) -->
<!-- ) -->

<!-- ggplot(pltGenInt,aes(x=x,y=y))+ -->
<!--   geom_line()+ -->
<!--   geom_ribbon(aes(ymin=ymin1,ymax=ymax1),alpha=0.1)+ -->
<!--   geom_ribbon(aes(ymin=ymin2,ymax=ymax2),alpha=0.15) -->

<!-- summary = generationInterval %>% group_by(bootstrapNumber) %>% mutate( -->
<!--   mean1 = sum(x * density) -->
<!--   ) %>% summarise( -->
<!--     mean1 = first(mean1), -->
<!--     sd1 = sqrt((x-mean1)^2 * generationInterval$density) -->
<!--   ) %>% ungroup() %>% -->
<!--   summarise( -->
<!--     meanOfMean = mean(mean1), -->
<!--     sdOfMean = sd(mean1), -->
<!--     lowerOfMean = quantile(mean1,0.025), -->
<!--     upperOfMean = quantile(mean1,0.975), -->
<!--     meanOfSd = mean(sd1), -->
<!--     sdOfSd = sd(sd1), -->
<!--     lowerOfSd = quantile(sd1,0.025), -->
<!--     upperOfSd = quantile(sd1,0.975), -->
<!--   ) %>%  -->
<!--   mutate( -->
<!--     `Mean \U00B1 SD and 95% CI` = sprintf("%1.2f \U00B1 %1.2f (%1.2f; %1.2f)",meanOfMean, sdOfMean, lowerOfMean, upperOfMean), -->
<!--     `SD \U00B1 SD and 95% CI` = sprintf("%1.2f \U00B1 %1.2f (%1.2f; %1.2f)",meanOfSd, sdOfSd, lowerOfSd, upperOfSd) -->
<!--   ) -->

<!-- ``` -->
<!-- ## With an offset parameter ---- -->
<!-- # TODO: to make this work properly we need to implement an offset gamma probability distribution family. -->

<!-- # estimateParams = function(errors,predicted) { -->
<!-- #    -->
<!-- #   gridSearch = function(shapeLim=c(0.1,5),rateLim=c(0.1,15),offsetLim=c(0,4), grid = NULL) { -->
<!-- #     #browser() -->
<!-- #     shapeWidth=(shapeLim[2]-shapeLim[1])/10 -->
<!-- #     rateWidth=(rateLim[2]-rateLim[1])/10 -->
<!-- #     offsetWidth=(offsetLim[2]-offsetLim[1])/10 -->
<!-- #     if(shapeWidth<0.0001) return(grid) -->
<!-- #     for(shape in seq(shapeLim[1]+shapeWidth,shapeLim[2]-shapeWidth,length.out = 5)) { -->
<!-- #       for(rate in seq(rateLim[1]+rateWidth,rateLim[2]-rateWidth,length.out = 5)) { -->
<!-- #         for(offset in seq(offsetLim[1]+offsetWidth,offsetLim[2]-offsetWidth,length.out = 5)) { -->
<!-- #           grid = grid %>% bind_rows( -->
<!-- #             tibble( -->
<!-- #               offset=offset, -->
<!-- #               shape=shape, -->
<!-- #               rate=rate, -->
<!-- #               mlse=simSi(shape=shape,rate=rate,offset=offset,errorVec=errors,predicted=predicted) -->
<!-- #             ) -->
<!-- #           ) -->
<!-- #         }   -->
<!-- #       } -->
<!-- #     } -->
<!-- #     gridmin = grid %>% filter(mlse==min(mlse)) -->
<!-- #     grid = gridmin %>% group_modify(function(d,g,..) { -->
<!-- #       return(gridSearch( -->
<!-- #         d$shape+c(-1,1)*shapeWidth, -->
<!-- #         d$rate+c(-1,1)*rateWidth, -->
<!-- #         d$offset+c(-1,1)*offsetWidth, -->
<!-- #         grid -->
<!-- #       )) -->
<!-- #     }) -->
<!-- #     return(grid %>% distinct()) -->
<!-- #   } -->
<!-- #    -->
<!-- #   return(gridSearch() %>% filter(mlse == min(mlse))) -->
<!-- # } -->
<!-- #  -->
<!-- # genInt2 = dpc$getHashCached(object = incubDist,operation = "GENERATION-INTERVAL-WITH-OFFSET",params = list(empiricalSis), orElse = function(...) { -->
<!-- #   return( -->
<!-- #     incubDist %>% group_by(bootstrapNumber) %>% group_modify(function(d,g,...) { -->
<!-- #         #browser() -->
<!-- #         errors = d$incubError -->
<!-- #         actual = empiricalSis %>% filter(bootstrapNumber==g$bootstrapNumber) -->
<!-- #         message(".",appendLF = FALSE) -->
<!-- #         return(estimateParams(errors=errors,predicted=actual$density)) -->
<!-- #     }) -->
<!-- #   ) -->
<!-- # }) -->
<!-- #  -->
<!-- # genInt2 = genInt2 %>% mutate(mean = shape/rate+offset, sd = sqrt(shape/(rate^2))) -->
<!-- #  -->
<!-- # genInt2Fit = DistributionFit$new() -->
<!-- # genInt2Tmp = genInt2 %>% select(bootstrapNumber,shape,rate) %>% pivot_longer(cols = c(shape,rate), names_to = "param", values_to = "value") %>% mutate(dist="gamma") -->
<!-- # genInt2Fit$fromBootstrappedDistributions(fittedDistributions = genInt2Tmp) -->
<!-- #  -->
<!-- # genInt2Fit$shifted = -mean(genInt2$offset) -->


<!-- ```{r} -->
<!-- empiricalSis = si1$bootstrapSamples %>% -->
<!--   group_by(bootstrapNumber) %>% -->
<!--   group_modify(function(d,g,...) { -->
<!--     tmp = density(d$value, from=-7, to=28,bw = 0.5) -->
<!--     return(tibble(value = tmp$x, density = tmp$y)) -->
<!--   }) -->

<!-- # TODO: predicted is assumed here to be a density of length 512 e.g. from lines above. -->
<!-- # could make this more generic and into an empirical probability distribution matching algorithm -->
<!-- simSi = function(shape, rate, offset, errorVec, predicted) { -->
<!--   #browser() -->
<!--   N = length(errorVec) -->
<!--   genSim = rgamma(N*100,shape,rate = rate)+offset+rep(errorVec,100) -->
<!--   tmp = density(genSim, from=-7, to=28,bw = 0.5) -->
<!--   return( -->
<!--     #tibble(value=tmp$x, density = tmp$y) -->
<!--     sqrt(mean((tmp$y - predicted)^2)) # this is RMSE - would log of RMSE be better? -->
<!--     # TODO: match moments rather than distributions? -->
<!--   ) -->
<!-- } -->

<!-- # For some reason the attempts to find these shape / rate / offset parameters fails -->
<!-- # when using optim -->
<!-- # # estimateParamsOptim = function(errors,actual) { -->
<!-- # -->
<!-- #   optimFunc = function(x) simSi(shape=x[1], rate=x[2], offset=x[3], errorVec = errors, predicted = actual) -->
<!-- #   out = optim(par = c(2,2,2), fn = optimFunc, lower = c(0,0,0), method = "L-BFGS-B") -->
<!-- #   TODO: assemble results into dataframe with shape, rate, offset. -->
<!-- # } -->
<!-- # this seems to hit local minima and get stuck -->
<!-- # Or possibly the first derivative is not well defined as depending on rgamma with shape and rate -->
<!-- # -->


<!-- ## Without an offset parameter ---- -->

<!-- estimateParamsNoOffset = function(errors,predicted) { -->

<!--   gridSearch = function(shapeLim=c(0.1,5),rateLim=c(0.1,15), grid = NULL) { -->
<!--     #browser() -->
<!--     shapeWidth=(shapeLim[2]-shapeLim[1])/10 -->
<!--     rateWidth=(rateLim[2]-rateLim[1])/10 -->
<!--     if(shapeWidth<0.0001) return(grid) -->
<!--     for(shape in seq(shapeLim[1]+shapeWidth,shapeLim[2]-shapeWidth,length.out = 5)) { -->
<!--       for(rate in seq(rateLim[1]+rateWidth,rateLim[2]-rateWidth,length.out = 5)) { -->
<!--         grid = grid %>% bind_rows( -->
<!--           tibble( -->
<!--             shape=shape, -->
<!--             rate=rate, -->
<!--             mlse=simSi(shape=shape,rate=rate,offset=0,errorVec=errors,predicted=predicted) -->
<!--           ) -->
<!--         ) -->
<!--       } -->
<!--     } -->
<!--     gridmin = grid %>% filter(mlse==min(mlse)) -->
<!--     grid = gridmin %>% group_modify(function(d,g,..) { -->
<!--       return(gridSearch( -->
<!--         d$shape+c(-1,1)*shapeWidth, -->
<!--         d$rate+c(-1,1)*rateWidth, -->
<!--         grid -->
<!--       )) -->
<!--     }) -->
<!--     return(grid %>% distinct()) -->
<!--   } -->

<!--   return(gridSearch() %>% filter(mlse == min(mlse)) %>% mutate(offset=0)) -->
<!-- } -->

<!-- # errors = incubDist %>% filter(bootstrapNumber==8) %>% pull(incubError) -->
<!-- # actual = empiricalSis %>% filter(bootstrapNumber==8) %>% pull(density) -->
<!-- # tmp = estimateParams(errors=errors,predicted=actual) -->

<!-- incubFit = bopFit$clone() -->
<!-- incubFit$filterModels(aic == min(aic)) -->
<!-- incubFit$bootstraps = incubFit$bootstraps %>% filter(bootstrapNumber <= 100) -->

<!-- # generate a set of samples from best fitting incubation period distribution -->
<!-- # get samples from incubation and split into 2 groups - one for index patient and one for affected patient -->
<!-- incubFit$generateSamples(sampleExpr = 2000,seed = 101) -->
<!-- incubSamples = incubFit$samples %>% -->
<!--   mutate(sampleCat = (sampleNumber-1) %/% 1000 + 1, sampleNumber = ((sampleNumber-1) %% 1000)+1) %>% -->
<!--   pivot_wider(names_from = sampleCat, values_from = value, names_prefix = "incub") -->
<!-- incubDist = incubSamples %>% mutate(delayOffset = incub2, incubError = incub1-incub2, transition = "infection to onset", from="infection", to = "onset") -->

<!-- genIntTmp = dpc$getHashCached(object = incubDist,operation = "GENERATION-INTERVAL-NO-OFFSET",params = list(empiricalSis), orElse = function(...) { -->
<!--   genInt = -->
<!--     incubDist %>% group_by(bootstrapNumber) %>% group_modify(function(d,g,...) { -->
<!--       #browser() -->
<!--       errors = d$incubError -->
<!--       actual = empiricalSis %>% filter(bootstrapNumber==g$bootstrapNumber) -->
<!--       message(".",appendLF = FALSE) -->
<!--       return(estimateParamsNoOffset(errors=errors,predicted=actual$density)) -->
<!--     }) -->
<!--   genInt = genInt %>% mutate(mean = shape/rate, sd = sqrt(shape/(rate^2))) -->
<!--   generationIntervalSimulation = genInt %>% select(bootstrapNumber,shape,rate) %>% pivot_longer(cols = c(shape,rate), names_to = "param", values_to = "value") %>% mutate(dist="gamma") -->
<!--   usethis::use_data(generationIntervalSimulation, overwrite = TRUE) -->
<!--   return(generationIntervalSimulation) -->
<!-- }) -->

<!-- si4 = SerialIntervalProvider$generationInterval(dpc,bootstrapsDf = genIntTmp) -->
<!-- ``` -->
